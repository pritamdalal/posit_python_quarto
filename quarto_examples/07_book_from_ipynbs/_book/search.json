[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python for Data Wrangling in Finance",
    "section": "",
    "text": "This is a Quarto book that I put together from an existing collection of Jupyter notebook tutorials."
  },
  {
    "objectID": "01_jumpstart.html#what-is-a-notebook",
    "href": "01_jumpstart.html#what-is-a-notebook",
    "title": "1  Python Jumpstart",
    "section": "1.1 What is a Notebook?",
    "text": "1.1 What is a Notebook?\nThis file - the one you are currently interacting with - is a Jupyter Notebook.\nThe notebook format conveniently allows you to combine words/sentences, computer code, code output (including plots), and mathematical notation. Notebooks have proven to be a convenient and productive programming environment for data analysis.\nFor those of you familiar with R, a Jupyter Notebook is similar in functionality to R Markdown notebooks.\nBehind the scenes of a Jupyter Notebook is a kernel that is responsible for executing computations. The kernel can live locally on your machine or on a remote server."
  },
  {
    "objectID": "01_jumpstart.html#code-cells",
    "href": "01_jumpstart.html#code-cells",
    "title": "1  Python Jumpstart",
    "section": "1.2 Code Cells",
    "text": "1.2 Code Cells\nA notebook is structured as a sequence of cells. There are two kinds of cells: 1) code cells that contain code; 2) markdown cells which contain markdown or latex.\nThe cell below is a code cell - try typing the code that is commented out and the press shift + enter.\n\nfrom IPython.display import Image\nImage(\"not_ethical.png\")"
  },
  {
    "objectID": "01_jumpstart.html#edit-mode-vs-command-mode",
    "href": "01_jumpstart.html#edit-mode-vs-command-mode",
    "title": "1  Python Jumpstart",
    "section": "1.3 Edit Mode vs Command Mode",
    "text": "1.3 Edit Mode vs Command Mode\nThere are two modes in a notebook: 1) edit mode; 2) command mode.\nIn edit mode you are inside a cell and you can edit the contents of the cell.\nIn command mode, you are outside the cells and you can navigate between them."
  },
  {
    "objectID": "01_jumpstart.html#keyboard-shortcuts",
    "href": "01_jumpstart.html#keyboard-shortcuts",
    "title": "1  Python Jumpstart",
    "section": "1.4 Keyboard Shortcuts",
    "text": "1.4 Keyboard Shortcuts\nHere are some of my favorite keyboard shortcuts:\nedit mode: enter\ncommand mode: esc\nnavigate up: k\nnavigate down: j\ninsert cell above: a\ninsert cell below: b\ndelete cell: d, d (press d twice)\nswitch to code cell: y\nswitch to markup cell: m\nexecute and stay on current cell: ctrl + enter\nexecute and move down a cell: shift + enter"
  },
  {
    "objectID": "01_jumpstart.html#drop-down-menus",
    "href": "01_jumpstart.html#drop-down-menus",
    "title": "1  Python Jumpstart",
    "section": "1.5 Drop Down Menus",
    "text": "1.5 Drop Down Menus\nHere are a few of the drop down menu functions that I use frequently:\nKernel > Restart Kernel and Clear All Outputs\nKernel > Restart Kearnel and Run All Cells\nRun > Run All Above Selected Cell"
  },
  {
    "objectID": "01_jumpstart.html#importing-packages",
    "href": "01_jumpstart.html#importing-packages",
    "title": "1  Python Jumpstart",
    "section": "1.6 Importing Packages",
    "text": "1.6 Importing Packages\nThe power and convenience of Python as a data analysis tool comes from the ecosystem of freely available third party packages.\nHere are the packages that we will be using in this tutorial:\nnumpy - efficient vector and matrix computations\npandas - working with DataFrames\npandas_datareader - reading data from Yahoo Finance\nThe following code imports these packages and assigns them each an alias.\n\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr"
  },
  {
    "objectID": "01_jumpstart.html#reading-in-stock-data-into-a-dataframe",
    "href": "01_jumpstart.html#reading-in-stock-data-into-a-dataframe",
    "title": "1  Python Jumpstart",
    "section": "1.7 Reading-In Stock Data into a DataFrame",
    "text": "1.7 Reading-In Stock Data into a DataFrame\nLet’s begin by reading in 5 years of SPY price data from Yahoo Finance.\nSPY is an ETF that tracks the performace of the SP500 stock index.\n\ndf_spy = pdr.get_data_yahoo('SPY', start='2014-01-01', end='2019-01-01')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2014-01-02\n      184.07\n      182.48\n      183.98\n      182.92\n      119636900.0\n      158.56\n    \n    \n      2014-01-03\n      183.60\n      182.63\n      183.23\n      182.89\n      81390600.0\n      158.54\n    \n    \n      2014-01-06\n      183.56\n      182.08\n      183.49\n      182.36\n      108028200.0\n      158.08\n    \n    \n      2014-01-07\n      183.79\n      182.95\n      183.09\n      183.48\n      86144200.0\n      159.05\n    \n    \n      2014-01-08\n      183.83\n      182.89\n      183.45\n      183.52\n      96582300.0\n      159.08\n    \n  \n\n\n\n\nOur stock data now lives in the variable called df_spy, which is a pandas data structure known as a DataFrame. We can see this by using the following code:\n\ntype(df_spy)\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "01_jumpstart.html#dataframe-index",
    "href": "01_jumpstart.html#dataframe-index",
    "title": "1  Python Jumpstart",
    "section": "1.8 DataFrame Index",
    "text": "1.8 DataFrame Index\nIn pandas, a DataFrame always has an index. For df_spy the Dates form the index.\n\ndf_spy.index\n\nDatetimeIndex(['2014-01-02', '2014-01-03', '2014-01-06', '2014-01-07',\n               '2014-01-08', '2014-01-09', '2014-01-10', '2014-01-13',\n               '2014-01-14', '2014-01-15',\n               ...\n               '2018-12-17', '2018-12-18', '2018-12-19', '2018-12-20',\n               '2018-12-21', '2018-12-24', '2018-12-26', '2018-12-27',\n               '2018-12-28', '2018-12-31'],\n              dtype='datetime64[ns]', name='Date', length=1258, freq=None)\n\n\nI don’t use indices very much, so let’s make the Date index just a regular column. Notice that we can modify DataFrames inplace.\n\ndf_spy.reset_index(inplace=True)\ndf_spy\n\n\n\n\n\n  \n    \n      \n      Date\n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n  \n  \n    \n      0\n      2014-01-02\n      184.07\n      182.48\n      183.98\n      182.92\n      119636900.0\n      158.56\n    \n    \n      1\n      2014-01-03\n      183.60\n      182.63\n      183.23\n      182.89\n      81390600.0\n      158.54\n    \n    \n      2\n      2014-01-06\n      183.56\n      182.08\n      183.49\n      182.36\n      108028200.0\n      158.08\n    \n    \n      3\n      2014-01-07\n      183.79\n      182.95\n      183.09\n      183.48\n      86144200.0\n      159.05\n    \n    \n      4\n      2014-01-08\n      183.83\n      182.89\n      183.45\n      183.52\n      96582300.0\n      159.08\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1253\n      2018-12-24\n      240.84\n      234.27\n      239.04\n      234.34\n      147311600.0\n      224.30\n    \n    \n      1254\n      2018-12-26\n      246.18\n      233.76\n      235.97\n      246.18\n      218485400.0\n      235.63\n    \n    \n      1255\n      2018-12-27\n      248.29\n      238.96\n      242.57\n      248.07\n      186267300.0\n      237.44\n    \n    \n      1256\n      2018-12-28\n      251.40\n      246.45\n      249.58\n      247.75\n      153100200.0\n      237.13\n    \n    \n      1257\n      2018-12-31\n      250.19\n      247.47\n      249.56\n      249.92\n      144299400.0\n      239.21\n    \n  \n\n1258 rows × 7 columns\n\n\n\nNotice that df_spy still has an index, now it’s just a sequence of integers.\n\ndf_spy.index\n\nRangeIndex(start=0, stop=1258, step=1)"
  },
  {
    "objectID": "01_jumpstart.html#a-bit-of-cleaning",
    "href": "01_jumpstart.html#a-bit-of-cleaning",
    "title": "1  Python Jumpstart",
    "section": "1.9 A Bit of Cleaning",
    "text": "1.9 A Bit of Cleaning\nAs a matter of preference, I like to make my column names snake case.\n\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ','_')\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2014-01-02\n      184.07\n      182.48\n      183.98\n      182.92\n      119636900.0\n      158.56\n    \n    \n      1\n      2014-01-03\n      183.60\n      182.63\n      183.23\n      182.89\n      81390600.0\n      158.54\n    \n    \n      2\n      2014-01-06\n      183.56\n      182.08\n      183.49\n      182.36\n      108028200.0\n      158.08\n    \n    \n      3\n      2014-01-07\n      183.79\n      182.95\n      183.09\n      183.48\n      86144200.0\n      159.05\n    \n    \n      4\n      2014-01-08\n      183.83\n      182.89\n      183.45\n      183.52\n      96582300.0\n      159.08\n    \n  \n\n\n\n\nLet’s also remove the columns that we won’t need. We first create a list of the column names that we want to get rid of and then we use the DataFrame.drop() method.\n\nlst_cols = ['high', 'low', 'open', 'close', 'volume',]\ndf_spy.drop(columns=lst_cols, inplace=True)\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      adj_close\n    \n  \n  \n    \n      0\n      2014-01-02\n      158.56\n    \n    \n      1\n      2014-01-03\n      158.54\n    \n    \n      2\n      2014-01-06\n      158.08\n    \n    \n      3\n      2014-01-07\n      159.05\n    \n    \n      4\n      2014-01-08\n      159.08\n    \n  \n\n\n\n\nNotice that trailing commas are not an issue in Python."
  },
  {
    "objectID": "01_jumpstart.html#series",
    "href": "01_jumpstart.html#series",
    "title": "1  Python Jumpstart",
    "section": "1.10 Series",
    "text": "1.10 Series\nYou can isolate the columns of a DataFrame with square brackets as follows:\n\ndf_spy['adj_close']\n\n0       158.56\n1       158.54\n2       158.08\n3       159.05\n4       159.08\n         ...  \n1253    224.30\n1254    235.63\n1255    237.44\n1256    237.13\n1257    239.21\nName: adj_close, Length: 1258, dtype: float64\n\n\nThe columns of a DataFrame are a pandas data structure called a Series.\n\ntype(df_spy['adj_close'])\n\npandas.core.series.Series"
  },
  {
    "objectID": "01_jumpstart.html#numpy-and-ndarrays",
    "href": "01_jumpstart.html#numpy-and-ndarrays",
    "title": "1  Python Jumpstart",
    "section": "1.11 numpy and ndarrays",
    "text": "1.11 numpy and ndarrays\nPython is a general purpose programming language and was not created for scientific computing in particular. One of the foundational packages that makes Python well suited to scientific computing is numpy, which has a variety of features including a data type called ndarrays. One of the benefits of ndarrays is that they allow for efficient vector and matrix computation.\nThe values of a Series object is a numpy.ndarray. This is one sense in which pandas is built on top of numpy.\n\ndf_spy['adj_close'].values\n\narray([158.56, 158.54, 158.08, ..., 237.44, 237.13, 239.21])\n\n\n\ntype(df_spy['adj_close'].values)\n\nnumpy.ndarray"
  },
  {
    "objectID": "01_jumpstart.html#series-built-in-methods",
    "href": "01_jumpstart.html#series-built-in-methods",
    "title": "1  Python Jumpstart",
    "section": "1.12 Series Built-In Methods",
    "text": "1.12 Series Built-In Methods\nSeries have a variety of built-in methods that provide convenient summarization and modification functionality. For example, you can .sum() all the elements of the Series.\n\ndf_spy['adj_close'].sum()\n\n259032.07\n\n\nNext, we calculate the standard deviation of all the elements of the Series.\n\ndf_spy['adj_close'].std()\n\n34.188328034526734\n\n\nThe .shift() built-in method will be useful for calculating returns in the next section.\n\ndf_spy['adj_close'].shift()\n\n0          NaN\n1       158.56\n2       158.54\n3       158.08\n4       159.05\n         ...  \n1253    230.39\n1254    224.30\n1255    235.63\n1256    237.44\n1257    237.13\nName: adj_close, Length: 1258, dtype: float64"
  },
  {
    "objectID": "01_jumpstart.html#calculating-daily-returns",
    "href": "01_jumpstart.html#calculating-daily-returns",
    "title": "1  Python Jumpstart",
    "section": "1.13 Calculating Daily Returns",
    "text": "1.13 Calculating Daily Returns\nOur analysis analysis of the leverage effect will involve daily returns for all the days in df_spy. Let’s calculate those now.\nRecall that the end-of-day day \\(t\\) return of a stock is defined as: \\(r_{t} = \\frac{S_{t}}{S_{t-1}} - 1\\), where \\(S_{t}\\) is the stock price at end-of-day \\(t\\).\nHere is a vectorized approach to calculating all the daily returns in a single line of code.\n\ndf_spy['ret'] = df_spy['adj_close'] / df_spy['adj_close'].shift(1) - 1\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      adj_close\n      ret\n    \n  \n  \n    \n      0\n      2014-01-02\n      158.56\n      NaN\n    \n    \n      1\n      2014-01-03\n      158.54\n      -0.000126\n    \n    \n      2\n      2014-01-06\n      158.08\n      -0.002901\n    \n    \n      3\n      2014-01-07\n      159.05\n      0.006136\n    \n    \n      4\n      2014-01-08\n      159.08\n      0.000189\n    \n  \n\n\n\n\nNotice that we can create a new column of a DataFrame by using variable assignment syntax."
  },
  {
    "objectID": "01_jumpstart.html#visualizing-adjusted-close-prices",
    "href": "01_jumpstart.html#visualizing-adjusted-close-prices",
    "title": "1  Python Jumpstart",
    "section": "1.14 Visualizing Adjusted Close Prices",
    "text": "1.14 Visualizing Adjusted Close Prices\nPython has a variety of packages that can be used for visualization. For this tutorial, we will focus on built-in plotting capabilities of pandas. These capabilities are built on top of the matplotlib package, which is the foundation of much of Python’s visualization ecosystem.\nDataFrames have a built-in .plot() method that makes creating simple line graphs quite easy.\n\ndf_spy.plot(x='date', y='adj_close');\n\n\n\n\nIf we wanted to make this graph more presentable we could do something like:\n\nax = df_spy.\\\n        plot(\n            x = 'date',\n            y = 'adj_close',\n            title = 'SPY: 2014-2018',\n            grid = True,\n            style = 'k',\n            alpha = 0.75,\n            figsize = (9, 4),\n        );\nax.set_xlabel('Trade Date');\nax.set_ylabel('Close Price');\n\n\n\n\nNotice that the ax variable created above is a matplotlib object.\n\ntype(ax)\n\nmatplotlib.axes._subplots.AxesSubplot"
  },
  {
    "objectID": "01_jumpstart.html#visualizing-returns",
    "href": "01_jumpstart.html#visualizing-returns",
    "title": "1  Python Jumpstart",
    "section": "1.15 Visualizing Returns",
    "text": "1.15 Visualizing Returns\nPandas also gives us the ability to simultaneously plot two different columns of a DataFrame in separate subplots of a single graph. Here is what that code looks like:\n\ndf_spy.plot(x='date', y=['adj_close', 'ret',], subplots=True, style='k', alpha=0.75, figsize=(9, 8), grid=True);\n\n\n\n\nThe returns graph above is a bit of a hack, but it’s used all the time in finance to demonstrate volatility clustering.\nNotice that whenever there is a sharp drop in the adj_close price graph, that the magnitude of the nearby returns becomes large. In contrast, during periods of steady growth (e.g. all of 2017) the magnitude of the returns is small."
  },
  {
    "objectID": "01_jumpstart.html#calculating-realized-volatility",
    "href": "01_jumpstart.html#calculating-realized-volatility",
    "title": "1  Python Jumpstart",
    "section": "1.16 Calculating Realized Volatility",
    "text": "1.16 Calculating Realized Volatility\nRealized volatility is defined as the standard deviation of the daily returns; it indicates how much variability in the stock price there has been. It is a matter of convention to annualize this quantity, so we multiply it by \\(\\sqrt{252}\\).\nThe following vectorized code calculates a rolling 2-month volatility for our SPY price data.\n\ndf_spy['ret'].rolling(42).std() * np.sqrt(252)\n\n0            NaN\n1            NaN\n2            NaN\n3            NaN\n4            NaN\n          ...   \n1253    0.226709\n1254    0.252811\n1255    0.249180\n1256    0.245997\n1257    0.247003\nName: ret, Length: 1258, dtype: float64\n\n\nLet’s add these realized volatility calculations todf_spy this with the following code:\n\ndf_spy['realized_vol'] = df_spy['ret'].rolling(42).std() * np.sqrt(252)\ndf_spy\n\n\n\n\n\n  \n    \n      \n      date\n      adj_close\n      ret\n      realized_vol\n    \n  \n  \n    \n      0\n      2014-01-02\n      158.56\n      NaN\n      NaN\n    \n    \n      1\n      2014-01-03\n      158.54\n      -0.000126\n      NaN\n    \n    \n      2\n      2014-01-06\n      158.08\n      -0.002901\n      NaN\n    \n    \n      3\n      2014-01-07\n      159.05\n      0.006136\n      NaN\n    \n    \n      4\n      2014-01-08\n      159.08\n      0.000189\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1253\n      2018-12-24\n      224.30\n      -0.026433\n      0.226709\n    \n    \n      1254\n      2018-12-26\n      235.63\n      0.050513\n      0.252811\n    \n    \n      1255\n      2018-12-27\n      237.44\n      0.007682\n      0.249180\n    \n    \n      1256\n      2018-12-28\n      237.13\n      -0.001306\n      0.245997\n    \n    \n      1257\n      2018-12-31\n      239.21\n      0.008772\n      0.247003\n    \n  \n\n1258 rows × 4 columns"
  },
  {
    "objectID": "01_jumpstart.html#visualizing-realized-volatility",
    "href": "01_jumpstart.html#visualizing-realized-volatility",
    "title": "1  Python Jumpstart",
    "section": "1.17 Visualizing Realized Volatility",
    "text": "1.17 Visualizing Realized Volatility\nWe can easily add realized_vol to our graph with the following code:\n\ndf_spy.plot(x = 'date', \n            y = ['adj_close','ret','realized_vol',], \n            subplots=True, style='k', alpha=0.75, \n            figsize=(9, 12), \n            grid=True);\n\n\n\n\nThis graph is an excellent illustration of the leverage effect. When SPY suffers losses, there is a spike in realized volatility, which is to say that the magnitude of the nearby returns increases."
  },
  {
    "objectID": "01_jumpstart.html#further-reading",
    "href": "01_jumpstart.html#further-reading",
    "title": "1  Python Jumpstart",
    "section": "1.18 Further Reading",
    "text": "1.18 Further Reading\nPython Data Science Handbook - Jake VanderPlas\nPython for Finance - Yves Hilpisch\nPython for Data Analysis - Wes McKinney\nAutomate the Boring Stuff - Al Sweigert"
  },
  {
    "objectID": "02_dataframe_basics.html#importing-packages",
    "href": "02_dataframe_basics.html#importing-packages",
    "title": "2  DataFrame Basics",
    "section": "2.1 Importing Packages",
    "text": "2.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport pandas_datareader as pdr\npd.set_option('display.max_rows', 10)"
  },
  {
    "objectID": "02_dataframe_basics.html#reading-in-data",
    "href": "02_dataframe_basics.html#reading-in-data",
    "title": "2  DataFrame Basics",
    "section": "2.2 Reading-In Data",
    "text": "2.2 Reading-In Data\nNext, let’s use pandas_datareader to read-in SPY prices from March 2020. SPY is an ETF that tracks the S&P500 index.\n\ndf_spy = pdr.get_data_yahoo('SPY', start='2020-02-28', end='2020-03-31')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n    \n    \n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n    \n    \n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n    \n    \n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n    \n    \n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98\n    \n  \n\n\n\n\nLet’s make the date a regular column, instead of an index, and also make the column names snake-case.\n\ndf_spy.reset_index(drop=False, inplace=True)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n    \n    \n      1\n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n    \n    \n      2\n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n    \n    \n      3\n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n    \n    \n      4\n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98"
  },
  {
    "objectID": "02_dataframe_basics.html#exploring-a-dataframe",
    "href": "02_dataframe_basics.html#exploring-a-dataframe",
    "title": "2  DataFrame Basics",
    "section": "2.3 Exploring a DataFrame",
    "text": "2.3 Exploring a DataFrame\nWe can explore our df_spy DataFrame in a variety of ways.\nFirst, we can first use the type() method to make sure what we have created is in fact a DataFrame.\n\ntype(df_spy)\n\npandas.core.frame.DataFrame\n\n\nNext, we can use the .dtypes attribute of the DataFrame to see the data types of each of the columns.\n\ndf_spy.dtypes\n\ndate         datetime64[ns]\nhigh                float64\nlow                 float64\nopen                float64\nclose               float64\nvolume              float64\nadj_close           float64\ndtype: object\n\n\nWe can also check the number of rows and columns by using the .shape attribute.\n\ndf_spy.shape\n\n(23, 7)\n\n\nAs we can see, our dataframe df_spy consists of 23 row and 7 columns.\nCode Challenge: Try the DataFrame.info() and DataFrame.describe() methods on df_spy.\n\ndf_spy.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 23 entries, 0 to 22\nData columns (total 7 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       23 non-null     datetime64[ns]\n 1   high       23 non-null     float64       \n 2   low        23 non-null     float64       \n 3   open       23 non-null     float64       \n 4   close      23 non-null     float64       \n 5   volume     23 non-null     float64       \n 6   adj_close  23 non-null     float64       \ndtypes: datetime64[ns](1), float64(6)\nmemory usage: 1.4 KB\n\n\n\ndf_spy.describe().round(2)\n\n\n\n\n\n  \n    \n      \n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      count\n      23.00\n      23.00\n      23.00\n      23.00\n      2.300000e+01\n      23.00\n    \n    \n      mean\n      272.47\n      258.70\n      264.83\n      266.16\n      2.743910e+08\n      260.07\n    \n    \n      std\n      24.93\n      26.36\n      25.84\n      26.98\n      6.239095e+07\n      25.96\n    \n    \n      min\n      229.68\n      218.26\n      228.19\n      222.95\n      1.713695e+08\n      218.72\n    \n    \n      25%\n      256.26\n      237.22\n      243.70\n      244.97\n      2.320808e+08\n      240.25\n    \n    \n      50%\n      263.33\n      251.05\n      256.00\n      261.20\n      2.764441e+08\n      256.24\n    \n    \n      75%\n      293.20\n      279.52\n      286.67\n      292.34\n      3.177212e+08\n      285.11\n    \n    \n      max\n      313.84\n      303.33\n      309.50\n      312.86\n      3.922207e+08\n      305.12"
  },
  {
    "objectID": "02_dataframe_basics.html#dataframe-columns",
    "href": "02_dataframe_basics.html#dataframe-columns",
    "title": "2  DataFrame Basics",
    "section": "2.4 DataFrame Columns",
    "text": "2.4 DataFrame Columns\nIn order to isolate a particular column we can use square brackets ([ ]). The following code isolates the close price column of df_spy:\n\ndf_spy['close']\n\n0     296.26\n1     309.09\n2     300.24\n3     312.86\n4     302.46\n       ...  \n18    246.79\n19    261.20\n20    253.42\n21    261.65\n22    257.75\nName: close, Length: 23, dtype: float64\n\n\nCode Challenge: Isolate the date column of df_spy.\n\ndf_spy['date']\n\n0    2020-02-28\n1    2020-03-02\n2    2020-03-03\n3    2020-03-04\n4    2020-03-05\n        ...    \n18   2020-03-25\n19   2020-03-26\n20   2020-03-27\n21   2020-03-30\n22   2020-03-31\nName: date, Length: 23, dtype: datetime64[ns]\n\n\nAs we can see from the following code, each column of a DataFrame is actually a different kind of pandas structure called a Series.\n\ntype(df_spy['close'])\n\npandas.core.series.Series\n\n\nHere is a bit of pandas inside baseball:\n\nA DataFrame is collection of columns that are glued together.\nEach column is a Series.\nA Series has two main components: 1) .values; 2) .index.\nThe .values component of a Series is a numpy.array.\n\nLet’s look at the .values attribute of the close column of df_spy:\n\ndf_spy['close'].values\n\narray([296.26, 309.09, 300.24, 312.86, 302.46, 297.46, 274.23, 288.42,\n       274.36, 248.11, 269.32, 239.85, 252.8 , 240.  , 240.51, 228.8 ,\n       222.95, 243.15, 246.79, 261.2 , 253.42, 261.65, 257.75])\n\n\nCode Challenge: Verify that the values component of the close column of df_spy is in fact a a numpy.array.\n\ntype(df_spy['close'].values)\n\nnumpy.ndarray"
  },
  {
    "objectID": "02_dataframe_basics.html#component-wise-column-operations",
    "href": "02_dataframe_basics.html#component-wise-column-operations",
    "title": "2  DataFrame Basics",
    "section": "2.5 Component-wise Column Operations",
    "text": "2.5 Component-wise Column Operations\nWe can perform component-wise (i.e. vector-like) calculations with DataFrame columns.\nThe following code divides all the close prices by 100.\n\ndf_spy['close'] / 100\n\n0     2.9626\n1     3.0909\n2     3.0024\n3     3.1286\n4     3.0246\n       ...  \n18    2.4679\n19    2.6120\n20    2.5342\n21    2.6165\n22    2.5775\nName: close, Length: 23, dtype: float64\n\n\nWe can also perform component-wise calculations between two colums.\nLet’s say we want to calculate the intraday range of SPY for each of the trade-dates in df_spy; this is the difference between the high and the low of each day. We can do this easily from the columns of our DataFrame.\n\ndf_spy['high'] - df_spy['low']\n\n0     12.35\n1     14.70\n2     16.27\n3      9.77\n4      8.46\n      ...  \n18    16.60\n19    13.75\n20     9.76\n21     8.90\n22     7.11\nLength: 23, dtype: float64\n\n\nCode Challenge: Calculate the difference between the close and open columns of df_spy.\n\ndf_spy['close'] - df_spy['open']\n\n0      7.56\n1     10.88\n2     -9.26\n3      6.74\n4     -2.52\n      ...  \n18     1.92\n19    11.68\n20     0.15\n21     5.95\n22    -2.81\nLength: 23, dtype: float64"
  },
  {
    "objectID": "02_dataframe_basics.html#adding-columns-via-variable-assignment",
    "href": "02_dataframe_basics.html#adding-columns-via-variable-assignment",
    "title": "2  DataFrame Basics",
    "section": "2.6 Adding Columns via Variable Assignment",
    "text": "2.6 Adding Columns via Variable Assignment\nLet’s say we want to save our intraday ranges back into df_spy for further analysis later. The most straight forward to do this is using variable assignment as follows:\n\ndf_spy['intraday_range'] = df_spy['high'] - df_spy['low']\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      intraday_range\n    \n  \n  \n    \n      0\n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n      12.35\n    \n    \n      1\n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n      14.70\n    \n    \n      2\n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n      16.27\n    \n    \n      3\n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n      9.77\n    \n    \n      4\n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98\n      8.46\n    \n  \n\n\n\n\nCode Challenge: Add a new column to df_spy called open_to_close that consists of the difference between the close and open of each day.\n\ndf_spy['open_to_close'] = df_spy['close'] - df_spy['open']\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      intraday_range\n      open_to_close\n    \n  \n  \n    \n      0\n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n      12.35\n      7.56\n    \n    \n      1\n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n      14.70\n      10.88\n    \n    \n      2\n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n      16.27\n      -9.26\n    \n    \n      3\n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n      9.77\n      6.74\n    \n    \n      4\n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98\n      8.46\n      -2.52"
  },
  {
    "objectID": "02_dataframe_basics.html#adding-columns-via-.assign",
    "href": "02_dataframe_basics.html#adding-columns-via-.assign",
    "title": "2  DataFrame Basics",
    "section": "2.7 Adding Columns via .assign()",
    "text": "2.7 Adding Columns via .assign()\nA powerful, but less intuitive of way of adding a column to a DataFrame uses the .assign() function, which makes use of lambda functions (i.e. anonymous functions).\nThe following code adds another column called intraday_range_assign.\n\ndf_spy.assign(intraday_range_assign = lambda df: df['high'] - df['low'])\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      intraday_range\n      open_to_close\n      intraday_range_assign\n    \n  \n  \n    \n      0\n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n      12.35\n      7.56\n      12.35\n    \n    \n      1\n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n      14.70\n      10.88\n      14.70\n    \n    \n      2\n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n      16.27\n      -9.26\n      16.27\n    \n    \n      3\n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n      9.77\n      6.74\n      9.77\n    \n    \n      4\n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98\n      8.46\n      -2.52\n      8.46\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      18\n      2020-03-25\n      256.35\n      239.75\n      244.87\n      246.79\n      299430300.0\n      242.10\n      16.60\n      1.92\n      16.60\n    \n    \n      19\n      2020-03-26\n      262.80\n      249.05\n      249.52\n      261.20\n      257632800.0\n      256.24\n      13.75\n      11.68\n      13.75\n    \n    \n      20\n      2020-03-27\n      260.81\n      251.05\n      253.27\n      253.42\n      224341200.0\n      248.61\n      9.76\n      0.15\n      9.76\n    \n    \n      21\n      2020-03-30\n      262.43\n      253.53\n      255.70\n      261.65\n      171369500.0\n      256.68\n      8.90\n      5.95\n      8.90\n    \n    \n      22\n      2020-03-31\n      263.33\n      256.22\n      260.56\n      257.75\n      194881100.0\n      252.85\n      7.11\n      -2.81\n      7.11\n    \n  \n\n23 rows × 10 columns\n\n\n\nCode Challenge: Verify that the column intraday_range_assign was not actually added to the df_spy.\n\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      intraday_range\n      open_to_close\n    \n  \n  \n    \n      0\n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n      12.35\n      7.56\n    \n    \n      1\n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n      14.70\n      10.88\n    \n    \n      2\n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n      16.27\n      -9.26\n    \n    \n      3\n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n      9.77\n      6.74\n    \n    \n      4\n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98\n      8.46\n      -2.52\n    \n  \n\n\n\n\nIn order to modify the original DataFrame we will need to reassign to the variable.\n\ndf_spy = df_spy.assign(intraday_range_assign = lambda df: df['high'] - df['low'])\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      intraday_range\n      open_to_close\n      intraday_range_assign\n    \n  \n  \n    \n      0\n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n      12.35\n      7.56\n      12.35\n    \n    \n      1\n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n      14.70\n      10.88\n      14.70\n    \n    \n      2\n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n      16.27\n      -9.26\n      16.27\n    \n    \n      3\n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n      9.77\n      6.74\n      9.77\n    \n    \n      4\n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98\n      8.46\n      -2.52\n      8.46\n    \n  \n\n\n\n\nCode Challenge: Use .assign() to create a new column in df_spy, call it open_to_close_assign, that contains the difference between the close and open.\n\ndf_spy = df_spy.assign(open_to_close_assign = lambda df: df['close'] - df['open'])\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      intraday_range\n      open_to_close\n      intraday_range_assign\n      open_to_close_assign\n    \n  \n  \n    \n      0\n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n      12.35\n      7.56\n      12.35\n      7.56\n    \n    \n      1\n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n      14.70\n      10.88\n      14.70\n      10.88\n    \n    \n      2\n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n      16.27\n      -9.26\n      16.27\n      -9.26\n    \n    \n      3\n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n      9.77\n      6.74\n      9.77\n      6.74\n    \n    \n      4\n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98\n      8.46\n      -2.52\n      8.46\n      -2.52"
  },
  {
    "objectID": "02_dataframe_basics.html#method-chaining",
    "href": "02_dataframe_basics.html#method-chaining",
    "title": "2  DataFrame Basics",
    "section": "2.8 Method Chaining",
    "text": "2.8 Method Chaining\nThe value of .assign() becomes clear when we start chaining methods together.\nIn order to see this, let’s first drop the columns that we created\n\nlst_cols = ['intraday_range', 'open_to_close', 'intraday_range_assign', 'open_to_close_assign']\ndf_spy.drop(columns=lst_cols, inplace=True)\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n    \n    \n      1\n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n    \n    \n      2\n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n    \n    \n      3\n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n    \n    \n      4\n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98\n    \n  \n\n\n\n\nThe following code adds the intraday and and open_to_close columns:\n\ndf_spy = \\\n    (\n    df_spy\n        .assign(intraday_range = lambda df: df['high'] - df['low'])\n        .assign(open_to_close = lambda df: df['close'] - df['open'])\n    )\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      intraday_range\n      open_to_close\n    \n  \n  \n    \n      0\n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n      12.35\n      7.56\n    \n    \n      1\n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n      14.70\n      10.88\n    \n    \n      2\n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n      16.27\n      -9.26\n    \n    \n      3\n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n      9.77\n      6.74\n    \n    \n      4\n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98\n      8.46\n      -2.52\n    \n  \n\n\n\n\nCode Challenge: Use .assign() to add a two new column to df_spy:\n\ndifference betwee the close and adj_close\nthe average of the low and open\n\n\ndf_spy = \\\n    (\n    df_spy\n        .assign(div = lambda df: df['close'] - df['adj_close'])\n        .assign(avg = lambda df: (df['low'] + df['open']) / 2)\n    )\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      intraday_range\n      open_to_close\n      div\n      avg\n    \n  \n  \n    \n      0\n      2020-02-28\n      297.89\n      285.54\n      288.70\n      296.26\n      384975800.0\n      288.93\n      12.35\n      7.56\n      7.33\n      287.120\n    \n    \n      1\n      2020-03-02\n      309.16\n      294.46\n      298.21\n      309.09\n      238703600.0\n      301.45\n      14.70\n      10.88\n      7.64\n      296.335\n    \n    \n      2\n      2020-03-03\n      313.84\n      297.57\n      309.50\n      300.24\n      300139100.0\n      292.82\n      16.27\n      -9.26\n      7.42\n      303.535\n    \n    \n      3\n      2020-03-04\n      313.10\n      303.33\n      306.12\n      312.86\n      176613400.0\n      305.12\n      9.77\n      6.74\n      7.74\n      304.725\n    \n    \n      4\n      2020-03-05\n      308.47\n      300.01\n      304.98\n      302.46\n      186366800.0\n      294.98\n      8.46\n      -2.52\n      7.48\n      302.495"
  },
  {
    "objectID": "02_dataframe_basics.html#aggregating-calulations-on-series",
    "href": "02_dataframe_basics.html#aggregating-calulations-on-series",
    "title": "2  DataFrame Basics",
    "section": "2.9 Aggregating Calulations on Series",
    "text": "2.9 Aggregating Calulations on Series\nSeries have a variety of built-in aggregation functions.\nFor example, we can use the following code to calculate the total SPY volume during March 2020:\n\ndf_spy['volume'].sum()\n\n6310993400.0\n\n\nHere some summary statistics on the intraday_range column that we added to our DataFrame earlier.\n\nprint(\"Mean: \", df_spy['intraday_range'].mean()) # average\nprint(\"St Dev:\", df_spy['intraday_range'].std()) # standard deviation\nprint(\"Min:\" , df_spy['intraday_range'].min()) # minimum\nprint(\"Max:\" , df_spy['intraday_range'].max()) # maximum\n\nMean:  13.774782608695652\nSt Dev: 4.430055273167614\nMin: 7.109999999999957\nMax: 22.960000000000008\n\n\nCode Challenge: Calculate the average daily volume for the trade dates in df_spy.\n\ndf_spy['volume'].mean()\n\n274391017.3913044"
  },
  {
    "objectID": "02_dataframe_basics.html#related-reading",
    "href": "02_dataframe_basics.html#related-reading",
    "title": "2  DataFrame Basics",
    "section": "2.10 Related Reading",
    "text": "2.10 Related Reading\nPDSH - Section 3.1 - Introducing Pandas Objects\nPDSH - Section 2.1 - Understanding Data Types in Python\nPDSH - Section 2.2 - The Basics of NumPy Arrays\nPDSH - Section 2.3 - Computation on NumPy Arrays: Universal Functions\nPDSH - Section 2.4 - Aggregations: Min, Max, and Everything In Between"
  },
  {
    "objectID": "03_index_slice.html#importing-packages",
    "href": "03_index_slice.html#importing-packages",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.1 Importing Packages",
    "text": "3.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr"
  },
  {
    "objectID": "03_index_slice.html#reading-in-data",
    "href": "03_index_slice.html#reading-in-data",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.2 Reading-In Data",
    "text": "3.2 Reading-In Data\nNext, lets grab some data from Yahoo finance. In particular, we’ll grab SPY price data from July 2021.\n\ndf_spy = pdr.get_data_yahoo('SPY', start='2021-06-30', end='2021-07-31')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n    \n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n    \n    \n      2021-07-02\n      434.10\n      430.52\n      431.67\n      433.72\n      57697700\n      433.72\n    \n    \n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n    \n    \n      2021-07-07\n      434.76\n      431.51\n      433.66\n      434.46\n      63549500\n      434.46\n    \n  \n\n\n\n\nThe following code resets the index so that date is a regular column; it also puts the column names into snake-case.\n\ndf_spy.reset_index(inplace=True)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n    \n      1\n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n    \n    \n      2\n      2021-07-02\n      434.10\n      430.52\n      431.67\n      433.72\n      57697700\n      433.72\n    \n    \n      3\n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n    \n    \n      4\n      2021-07-07\n      434.76\n      431.51\n      433.66\n      434.46\n      63549500\n      434.46\n    \n  \n\n\n\n\nIt is often useful to look at the data type of each of the columns of a new data set. We can do so with the DataFrame.dtypes attribute.\n\ndf_spy.dtypes\n\ndate         datetime64[ns]\nhigh                float64\nlow                 float64\nopen                float64\nclose               float64\nvolume                int64\nadj_close           float64\ndtype: object"
  },
  {
    "objectID": "03_index_slice.html#row-slicing",
    "href": "03_index_slice.html#row-slicing",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.3 Row Slicing",
    "text": "3.3 Row Slicing\nThe simplest way to slice a DataFrame is to use square brackets: []. The syntax df[i:j] will generate a DataFrame who’s first row is the ith row of df and who’s last row is the (j-1)th row of df. Let’s demonstrate this with a some examples:\nStarting from the 0th row, and ending with the 0th row:\n\ndf_spy[0:1]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n  \n\n\n\n\nStarting with the 3rd row, and ending with the 6th row:\n\ndf_spy[3:7]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      3\n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n    \n    \n      4\n      2021-07-07\n      434.76\n      431.51\n      433.66\n      434.46\n      63549500\n      434.46\n    \n    \n      5\n      2021-07-08\n      431.73\n      427.52\n      428.78\n      430.92\n      97595200\n      430.92\n    \n    \n      6\n      2021-07-09\n      435.84\n      430.71\n      432.53\n      435.52\n      76238600\n      435.52\n    \n  \n\n\n\n\nCode Challenge: Retrieve the 15th, 16th, and 17th rows of df_spy.\n\ndf_spy[15:18]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      15\n      2021-07-22\n      435.72\n      433.69\n      434.74\n      435.46\n      47878500\n      435.46\n    \n    \n      16\n      2021-07-23\n      440.30\n      436.79\n      437.52\n      439.94\n      63766600\n      439.94\n    \n    \n      17\n      2021-07-26\n      441.03\n      439.26\n      439.31\n      441.02\n      43719200\n      441.02\n    \n  \n\n\n\n\nUsing the syntax df[:n] automatically starts the indexing at 0. For example, the following code retrieves all of df_spy (notice that len(df_spy) gives the number of rows of df_spy):\n\ndf_spy[:len(df_spy)]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n    \n      1\n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n    \n    \n      2\n      2021-07-02\n      434.10\n      430.52\n      431.67\n      433.72\n      57697700\n      433.72\n    \n    \n      3\n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n    \n    \n      4\n      2021-07-07\n      434.76\n      431.51\n      433.66\n      434.46\n      63549500\n      434.46\n    \n    \n      5\n      2021-07-08\n      431.73\n      427.52\n      428.78\n      430.92\n      97595200\n      430.92\n    \n    \n      6\n      2021-07-09\n      435.84\n      430.71\n      432.53\n      435.52\n      76238600\n      435.52\n    \n    \n      7\n      2021-07-12\n      437.35\n      434.97\n      435.43\n      437.08\n      52889600\n      437.08\n    \n    \n      8\n      2021-07-13\n      437.84\n      435.31\n      436.24\n      435.59\n      52911300\n      435.59\n    \n    \n      9\n      2021-07-14\n      437.92\n      434.91\n      437.40\n      436.24\n      64130400\n      436.24\n    \n    \n      10\n      2021-07-15\n      435.53\n      432.72\n      434.81\n      434.75\n      55126400\n      434.75\n    \n    \n      11\n      2021-07-16\n      436.06\n      430.92\n      436.01\n      431.34\n      75874700\n      431.34\n    \n    \n      12\n      2021-07-19\n      431.41\n      421.97\n      426.19\n      424.97\n      147987000\n      424.97\n    \n    \n      13\n      2021-07-20\n      432.42\n      424.83\n      425.68\n      431.06\n      99608200\n      431.06\n    \n    \n      14\n      2021-07-21\n      434.70\n      431.01\n      432.34\n      434.55\n      64724400\n      434.55\n    \n    \n      15\n      2021-07-22\n      435.72\n      433.69\n      434.74\n      435.46\n      47878500\n      435.46\n    \n    \n      16\n      2021-07-23\n      440.30\n      436.79\n      437.52\n      439.94\n      63766600\n      439.94\n    \n    \n      17\n      2021-07-26\n      441.03\n      439.26\n      439.31\n      441.02\n      43719200\n      441.02\n    \n    \n      18\n      2021-07-27\n      439.94\n      435.99\n      439.91\n      439.01\n      67397100\n      439.01\n    \n    \n      19\n      2021-07-28\n      440.30\n      437.31\n      439.68\n      438.83\n      52472400\n      438.83\n    \n    \n      20\n      2021-07-29\n      441.80\n      439.81\n      439.82\n      440.65\n      47435300\n      440.65\n    \n    \n      21\n      2021-07-30\n      440.06\n      437.77\n      437.91\n      438.51\n      68890600\n      438.51\n    \n  \n\n\n\n\nCode Challenge: Retrieve the first five rows of df_spy.\n\ndf_spy[:5]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n    \n      1\n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n    \n    \n      2\n      2021-07-02\n      434.10\n      430.52\n      431.67\n      433.72\n      57697700\n      433.72\n    \n    \n      3\n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n    \n    \n      4\n      2021-07-07\n      434.76\n      431.51\n      433.66\n      434.46\n      63549500\n      434.46\n    \n  \n\n\n\n\nThere are a couple of row slicing tricks that involve negative numbers that are worth mentioning.\nThe syntax df[-n:] retrieves the last n rows of df. The following code retrieves the last five rows of df_spy.\n\ndf_spy[-5:]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      17\n      2021-07-26\n      441.03\n      439.26\n      439.31\n      441.02\n      43719200\n      441.02\n    \n    \n      18\n      2021-07-27\n      439.94\n      435.99\n      439.91\n      439.01\n      67397100\n      439.01\n    \n    \n      19\n      2021-07-28\n      440.30\n      437.31\n      439.68\n      438.83\n      52472400\n      438.83\n    \n    \n      20\n      2021-07-29\n      441.80\n      439.81\n      439.82\n      440.65\n      47435300\n      440.65\n    \n    \n      21\n      2021-07-30\n      440.06\n      437.77\n      437.91\n      438.51\n      68890600\n      438.51\n    \n  \n\n\n\n\nThe syntax df[:-n] retrieves all but the last n rows of df. The following code retrieves all but the last 10 rows of df_spy:\n\ndf_spy[:-10]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n    \n      1\n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n    \n    \n      2\n      2021-07-02\n      434.10\n      430.52\n      431.67\n      433.72\n      57697700\n      433.72\n    \n    \n      3\n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n    \n    \n      4\n      2021-07-07\n      434.76\n      431.51\n      433.66\n      434.46\n      63549500\n      434.46\n    \n    \n      5\n      2021-07-08\n      431.73\n      427.52\n      428.78\n      430.92\n      97595200\n      430.92\n    \n    \n      6\n      2021-07-09\n      435.84\n      430.71\n      432.53\n      435.52\n      76238600\n      435.52\n    \n    \n      7\n      2021-07-12\n      437.35\n      434.97\n      435.43\n      437.08\n      52889600\n      437.08\n    \n    \n      8\n      2021-07-13\n      437.84\n      435.31\n      436.24\n      435.59\n      52911300\n      435.59\n    \n    \n      9\n      2021-07-14\n      437.92\n      434.91\n      437.40\n      436.24\n      64130400\n      436.24\n    \n    \n      10\n      2021-07-15\n      435.53\n      432.72\n      434.81\n      434.75\n      55126400\n      434.75\n    \n    \n      11\n      2021-07-16\n      436.06\n      430.92\n      436.01\n      431.34\n      75874700\n      431.34\n    \n  \n\n\n\n\nCode Challenge: Retrieve the first row of df_spy with negative indexing.\n\ndf_spy[:-(len(df_spy)-1)]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n  \n\n\n\n\nCode Challenge: Use simple slicing to select the last three rows of a df_spy without explicitly using row numbers.\n\ndf_spy[len(df_spy)-3:len(df_spy)]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      19\n      2021-07-28\n      440.30\n      437.31\n      439.68\n      438.83\n      52472400\n      438.83\n    \n    \n      20\n      2021-07-29\n      441.80\n      439.81\n      439.82\n      440.65\n      47435300\n      440.65\n    \n    \n      21\n      2021-07-30\n      440.06\n      437.77\n      437.91\n      438.51\n      68890600\n      438.51\n    \n  \n\n\n\n\n\ndf_spy[-3:]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      19\n      2021-07-28\n      440.30\n      437.31\n      439.68\n      438.83\n      52472400\n      438.83\n    \n    \n      20\n      2021-07-29\n      441.80\n      439.81\n      439.82\n      440.65\n      47435300\n      440.65\n    \n    \n      21\n      2021-07-30\n      440.06\n      437.77\n      437.91\n      438.51\n      68890600\n      438.51"
  },
  {
    "objectID": "03_index_slice.html#dataframe-indexes",
    "href": "03_index_slice.html#dataframe-indexes",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.4 DataFrame Indexes",
    "text": "3.4 DataFrame Indexes\nUnder the hood, a DataFrame has several indexes:\ncolumns - the set of column names is an (explicit) index.\nrow - whenever a DataFrame is created, there is an explicit row index that is created. If one isn’t specified, then a sequence of non-negative integers is used.\nimplicit - each row has an implicit row-number, and each column has an implicit column-number.\nLet’s take a look at the columns index of df_spy:\n\ndf_spy.columns\n\nIndex(['date', 'high', 'low', 'open', 'close', 'volume', 'adj_close'], dtype='object')\n\n\n\ntype(df_spy.columns)\n\npandas.core.indexes.base.Index\n\n\nNext, let’s take a look at the explicit row index attribute of df_spy:\n\ndf_spy.index\n\nRangeIndex(start=0, stop=22, step=1)\n\n\n\ntype(df_spy.index)\n\npandas.core.indexes.range.RangeIndex\n\n\nSince we reset the index for df_spy, a RangeIndex object is used for the explicit row index. You can think of a RangeIndex object as a glorified set of consecutive integers.\nFor the most part, we won’t be too concerned with indexes. A lot of data analysis can be done without worrying about them. However, it’s good to be aware indexes exist becase they can come into play for more advanced topics, such as joining tables together; they also come up in Stack Overflow examples frequently.\nFor the purposes of this tutorial, our interest in indexes comes from how they are related to two built-in DataFrame indexers: DataFrame.iloc and DataFrame.loc."
  },
  {
    "objectID": "03_index_slice.html#indexing-with-dataframe.iloc",
    "href": "03_index_slice.html#indexing-with-dataframe.iloc",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.5 Indexing with DataFrame.iloc",
    "text": "3.5 Indexing with DataFrame.iloc\nThe indexer DataFrame.iloc can be used to access rows and columns using their implicit row and column numbers.\nHere is an example of iloc that retrieves the first two rows of df_spy:\n\ndf_spy.iloc[0:2,]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n    \n      1\n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n    \n  \n\n\n\n\nNotice, that because we didn’t specify any column numbers, the code above retrieves all columns.\nThe following code grabs the first three row and the first three columns of df_spy:\n\ndf_spy.iloc[0:3, 0:3]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n    \n    \n      1\n      2021-07-01\n      430.60\n      428.80\n    \n    \n      2\n      2021-07-02\n      434.10\n      430.52\n    \n  \n\n\n\n\nWe can also supply .iloc with lists rather than ranges to specify custom sets of columns and rows:\n\nlst_row = [0, 2] # 0th and 2nd row\nlst_col = [0, 6] # date and adj_close columns\ndf_spy.iloc[lst_row, lst_col]\n\n\n\n\n\n  \n    \n      \n      date\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.06\n    \n    \n      2\n      2021-07-02\n      433.72\n    \n  \n\n\n\n\nUsing lists as a means of indexing is sometimes referred to as fancy indexing.\nCode Challenge Use fancy indexing to grab the 14th, 0th, and 5th rows of df_spy - in that order.\n\ndf_spy.iloc[[14, 0, 5]]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      14\n      2021-07-21\n      434.70\n      431.01\n      432.34\n      434.55\n      64724400\n      434.55\n    \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n    \n      5\n      2021-07-08\n      431.73\n      427.52\n      428.78\n      430.92\n      97595200\n      430.92"
  },
  {
    "objectID": "03_index_slice.html#indexing-with-dataframe.loc",
    "href": "03_index_slice.html#indexing-with-dataframe.loc",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.6 Indexing with DataFrame.loc",
    "text": "3.6 Indexing with DataFrame.loc\nRather than using the implicit row or column numbers, it is often more useful to access data by using the explicit row or column indices.\nLet’s use the DataFrame.set_index() method to set the date column as our new index. The dates will be a more interesting explicit index.\n\ndf_spy.set_index('date', inplace = True)\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n    \n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n    \n    \n      2021-07-02\n      434.10\n      430.52\n      431.67\n      433.72\n      57697700\n      433.72\n    \n    \n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n    \n    \n      2021-07-07\n      434.76\n      431.51\n      433.66\n      434.46\n      63549500\n      434.46\n    \n  \n\n\n\n\nTo see the effect of the above code, we can have a look at the index of df_spy.\n\ndf_spy.index\n\nDatetimeIndex(['2021-06-30', '2021-07-01', '2021-07-02', '2021-07-06',\n               '2021-07-07', '2021-07-08', '2021-07-09', '2021-07-12',\n               '2021-07-13', '2021-07-14', '2021-07-15', '2021-07-16',\n               '2021-07-19', '2021-07-20', '2021-07-21', '2021-07-22',\n               '2021-07-23', '2021-07-26', '2021-07-27', '2021-07-28',\n               '2021-07-29', '2021-07-30'],\n              dtype='datetime64[ns]', name='date', freq=None)\n\n\nAnd notice that date is no longer column of df_spy:\n\ndf_spy.columns\n\nIndex(['high', 'low', 'open', 'close', 'volume', 'adj_close'], dtype='object')\n\n\nNow that we have successfully set the row index of df_spy to be the date, let’s see how we can use this index to access the data via .loc.\nHere is an example of how we can grab a slice of rows, associated with a date-range:\n\ndf_spy.loc['2021-07-23':'2021-07-31']\n\n\n\n\n\n  \n    \n      \n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-07-23\n      440.30\n      436.79\n      437.52\n      439.94\n      63766600\n      439.94\n    \n    \n      2021-07-26\n      441.03\n      439.26\n      439.31\n      441.02\n      43719200\n      441.02\n    \n    \n      2021-07-27\n      439.94\n      435.99\n      439.91\n      439.01\n      67397100\n      439.01\n    \n    \n      2021-07-28\n      440.30\n      437.31\n      439.68\n      438.83\n      52472400\n      438.83\n    \n    \n      2021-07-29\n      441.80\n      439.81\n      439.82\n      440.65\n      47435300\n      440.65\n    \n    \n      2021-07-30\n      440.06\n      437.77\n      437.91\n      438.51\n      68890600\n      438.51\n    \n  \n\n\n\n\nIf we want to select only the volume and adjusted columns for these dates, we would type the following:\n\ndf_spy.loc['2021-07-23':'2021-07-31', ['volume', 'adj_close']]\n\n\n\n\n\n  \n    \n      \n      volume\n      adj_close\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      2021-07-23\n      63766600\n      439.94\n    \n    \n      2021-07-26\n      43719200\n      441.02\n    \n    \n      2021-07-27\n      67397100\n      439.01\n    \n    \n      2021-07-28\n      52472400\n      438.83\n    \n    \n      2021-07-29\n      47435300\n      440.65\n    \n    \n      2021-07-30\n      68890600\n      438.51\n    \n  \n\n\n\n\nCode Challenge: Use .loc to grab the date, volume, and close columns from df_spy.\n\ndf_spy.loc[:,['volume', 'close']]\n\n\n\n\n\n  \n    \n      \n      volume\n      close\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      2021-06-30\n      64827900\n      428.06\n    \n    \n      2021-07-01\n      53441000\n      430.43\n    \n    \n      2021-07-02\n      57697700\n      433.72\n    \n    \n      2021-07-06\n      68710400\n      432.93\n    \n    \n      2021-07-07\n      63549500\n      434.46\n    \n    \n      2021-07-08\n      97595200\n      430.92\n    \n    \n      2021-07-09\n      76238600\n      435.52\n    \n    \n      2021-07-12\n      52889600\n      437.08\n    \n    \n      2021-07-13\n      52911300\n      435.59\n    \n    \n      2021-07-14\n      64130400\n      436.24\n    \n    \n      2021-07-15\n      55126400\n      434.75\n    \n    \n      2021-07-16\n      75874700\n      431.34\n    \n    \n      2021-07-19\n      147987000\n      424.97\n    \n    \n      2021-07-20\n      99608200\n      431.06\n    \n    \n      2021-07-21\n      64724400\n      434.55\n    \n    \n      2021-07-22\n      47878500\n      435.46\n    \n    \n      2021-07-23\n      63766600\n      439.94\n    \n    \n      2021-07-26\n      43719200\n      441.02\n    \n    \n      2021-07-27\n      67397100\n      439.01\n    \n    \n      2021-07-28\n      52472400\n      438.83\n    \n    \n      2021-07-29\n      47435300\n      440.65\n    \n    \n      2021-07-30\n      68890600\n      438.51"
  },
  {
    "objectID": "03_index_slice.html#related-reading",
    "href": "03_index_slice.html#related-reading",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.7 Related Reading",
    "text": "3.7 Related Reading\nPDSH - 2.7 - Fancy Indexing\nPDSH - 3.2 - Data Indexing and Selection"
  },
  {
    "objectID": "04_query.html#importing-packages",
    "href": "04_query.html#importing-packages",
    "title": "4  DataFrame Querying",
    "section": "4.1 Importing Packages",
    "text": "4.1 Importing Packages\nLet’s first import the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr"
  },
  {
    "objectID": "04_query.html#reading-in-data",
    "href": "04_query.html#reading-in-data",
    "title": "4  DataFrame Querying",
    "section": "4.2 Reading-In Data",
    "text": "4.2 Reading-In Data\nNext, let’s use pandas_datareader to read-in some SPY data from July 2021.\n\ndf_spy = pdr.get_data_yahoo('SPY', start='2021-06-30', end='2021-07-31')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n    \n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n    \n    \n      2021-07-02\n      434.10\n      430.52\n      431.67\n      433.72\n      57697700\n      433.72\n    \n    \n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n    \n    \n      2021-07-07\n      434.76\n      431.51\n      433.66\n      434.46\n      63549500\n      434.46\n    \n  \n\n\n\n\nThe following code resets the index so that date is a regular column, and then makes the all column names snake-case.\n\ndf_spy.reset_index(inplace=True)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.head()\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n    \n    \n      1\n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n    \n    \n      2\n      2021-07-02\n      434.10\n      430.52\n      431.67\n      433.72\n      57697700\n      433.72\n    \n    \n      3\n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n    \n    \n      4\n      2021-07-07\n      434.76\n      431.51\n      433.66\n      434.46\n      63549500\n      434.46"
  },
  {
    "objectID": "04_query.html#comparison-and-dataframe-columns",
    "href": "04_query.html#comparison-and-dataframe-columns",
    "title": "4  DataFrame Querying",
    "section": "4.3 Comparison and DataFrame Columns",
    "text": "4.3 Comparison and DataFrame Columns\nAs discussed in a previous tutorial, a column of a DataFrame is a Series object, which is a souped up numpy.array (think vector or matrix).\nLet’s separate out the adjusted column of df_spy and assign it to a variable:\n\npd.options.display.max_rows = 6 # this modifies the printing of dataframes\nser_adjusted = df_spy['adj_close']\nser_adjusted\n\n0     428.06\n1     430.43\n2     433.72\n       ...  \n19    438.83\n20    440.65\n21    438.51\nName: adj_close, Length: 22, dtype: float64\n\n\nRecall that a pandas.Series is smart with respect to component-wise arithmetic operations, meaning it behaves like a vector from linear algebra. This means that arithmetic operations are broadcasted as you might expect.\nFor example, division by 100 is broadcasted component-wise:\n\nser_adjusted / 100\n\n0     4.2806\n1     4.3043\n2     4.3372\n       ...  \n19    4.3883\n20    4.4065\n21    4.3851\nName: adj_close, Length: 22, dtype: float64\n\n\nIt is a convenient fact that this broadcasting behavior also occurs with comparison, and produces a Series of booleans.\nThe following code checks which elements of ser_adjusted are greater than 435:\n\nser_test = (ser_adjusted > 435)\nser_test\n\n0     False\n1     False\n2     False\n      ...  \n19     True\n20     True\n21     True\nName: adj_close, Length: 22, dtype: bool\n\n\nLet’s check that the resulting variable ser_test is a pandas.Series:\n\ntype(ser_test)\n\npandas.core.series.Series\n\n\nAnd finally let’s observe the .values elements of ser_test:\n\nprint(ser_test.values)\n\n[False False False False False False  True  True  True  True False False\n False False False  True  True  True  True  True  True  True]\n\n\nA few observation about what just happened:\n\nWhen we compare a Series of numerical values (ser_adjusted) to a single number (435), we get back a Series of booleans (ser_test).\nWe have that ser_test[i] = (ser_adjusted[i] > 435).\nSo the comparison operation was broadcasted as advertised.\n\nThis is easy to see by appending ser_test to df_spy and then reprinting:\n\npd.options.display.max_rows = 25\ndf_spy['test'] = ser_test\ndf_spy\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      test\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n      False\n    \n    \n      1\n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n      False\n    \n    \n      2\n      2021-07-02\n      434.10\n      430.52\n      431.67\n      433.72\n      57697700\n      433.72\n      False\n    \n    \n      3\n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n      False\n    \n    \n      4\n      2021-07-07\n      434.76\n      431.51\n      433.66\n      434.46\n      63549500\n      434.46\n      False\n    \n    \n      5\n      2021-07-08\n      431.73\n      427.52\n      428.78\n      430.92\n      97595200\n      430.92\n      False\n    \n    \n      6\n      2021-07-09\n      435.84\n      430.71\n      432.53\n      435.52\n      76238600\n      435.52\n      True\n    \n    \n      7\n      2021-07-12\n      437.35\n      434.97\n      435.43\n      437.08\n      52889600\n      437.08\n      True\n    \n    \n      8\n      2021-07-13\n      437.84\n      435.31\n      436.24\n      435.59\n      52911300\n      435.59\n      True\n    \n    \n      9\n      2021-07-14\n      437.92\n      434.91\n      437.40\n      436.24\n      64130400\n      436.24\n      True\n    \n    \n      10\n      2021-07-15\n      435.53\n      432.72\n      434.81\n      434.75\n      55126400\n      434.75\n      False\n    \n    \n      11\n      2021-07-16\n      436.06\n      430.92\n      436.01\n      431.34\n      75874700\n      431.34\n      False\n    \n    \n      12\n      2021-07-19\n      431.41\n      421.97\n      426.19\n      424.97\n      147987000\n      424.97\n      False\n    \n    \n      13\n      2021-07-20\n      432.42\n      424.83\n      425.68\n      431.06\n      99608200\n      431.06\n      False\n    \n    \n      14\n      2021-07-21\n      434.70\n      431.01\n      432.34\n      434.55\n      64724400\n      434.55\n      False\n    \n    \n      15\n      2021-07-22\n      435.72\n      433.69\n      434.74\n      435.46\n      47878500\n      435.46\n      True\n    \n    \n      16\n      2021-07-23\n      440.30\n      436.79\n      437.52\n      439.94\n      63766600\n      439.94\n      True\n    \n    \n      17\n      2021-07-26\n      441.03\n      439.26\n      439.31\n      441.02\n      43719200\n      441.02\n      True\n    \n    \n      18\n      2021-07-27\n      439.94\n      435.99\n      439.91\n      439.01\n      67397100\n      439.01\n      True\n    \n    \n      19\n      2021-07-28\n      440.30\n      437.31\n      439.68\n      438.83\n      52472400\n      438.83\n      True\n    \n    \n      20\n      2021-07-29\n      441.80\n      439.81\n      439.82\n      440.65\n      47435300\n      440.65\n      True\n    \n    \n      21\n      2021-07-30\n      440.06\n      437.77\n      437.91\n      438.51\n      68890600\n      438.51\n      True\n    \n  \n\n\n\n\nAs we will see in the next two sections, the broadcasting of comparison can be used to query subsets of rows of a DataFrame."
  },
  {
    "objectID": "04_query.html#dataframe-masking",
    "href": "04_query.html#dataframe-masking",
    "title": "4  DataFrame Querying",
    "section": "4.4 DataFrame Masking",
    "text": "4.4 DataFrame Masking\nFrom the code below we know that df_spy has 22 rows:\n\ndf_spy.shape\n\n(22, 8)\n\n\nThe following code creates a list consisting of 22 booleans, all of them False:\n\nlst_bool = [False] * 22\nlst_bool\n\n[False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False]\n\n\nNow, let’s see what happens when we feed this list of False booleans into df_spy using square brackets.\n\ndf_spy[lst_bool]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      test\n    \n  \n  \n  \n\n\n\n\nCode Challenge: Verify that df_spy[lst_bool] is an empty DataFrame.\n\ntype(df_spy[lst_bool])\n\npandas.core.frame.DataFrame\n\n\n\ndf_spy[lst_bool].shape\n\n(0, 8)\n\n\nNext let’s modify lst_bool slightly, by changing the 0th entry to True, and then feed it into df_spy again.\n\nlst_bool[0] = True\ndf_spy[lst_bool]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      test\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n      False\n    \n  \n\n\n\n\nSo what happened? Notice that df_spy[lst_bool] returns a DataFrame consisting only of the 0th row of df_spy.\nLet’s modify lst_bool once again, by setting the 1st entry of df_spy to True, and then once again feed it into df_spy.\n\nlst_bool[1] = True\ndf_spy[lst_bool]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      test\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n      False\n    \n    \n      1\n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n      False\n    \n  \n\n\n\n\nPunchline: What is returned by the code df_spy[lst_bool] will be a DataFrame consisting of all the rows corresponding to the True entries of lst_bool.\nThis is called DataFrame masking.\nCode Challenge: Modify lst_bool and then use DataFrame masking to grab the 0th, 1st and, 3rd rows of df_spy.\n\nlst_bool[3] = True\ndf_spy[lst_bool]\n\n\n\n\n\n  \n    \n      \n      date\n      high\n      low\n      open\n      close\n      volume\n      adj_close\n      test\n    \n  \n  \n    \n      0\n      2021-06-30\n      428.78\n      427.18\n      427.21\n      428.06\n      64827900\n      428.06\n      False\n    \n    \n      1\n      2021-07-01\n      430.60\n      428.80\n      428.87\n      430.43\n      53441000\n      430.43\n      False\n    \n    \n      3\n      2021-07-06\n      434.01\n      430.01\n      433.78\n      432.93\n      68710400\n      432.93\n      False"
  },
  {
    "objectID": "04_query.html#querying-with-dataframe-masking",
    "href": "04_query.html#querying-with-dataframe-masking",
    "title": "4  DataFrame Querying",
    "section": "4.5 Querying with DataFrame Masking",
    "text": "4.5 Querying with DataFrame Masking\nWe often want to query a DataFrame based on some kind of comparison involving its column values.\nWe can achieve this kind of querying by combining the broadcasting of camparison over DataFrame columns with DataFrame masking.\nIn order to consider concrete examples, let’s read-in some data.\nThe following code reads in a dataset consisting of EOD prices for four different ETFs (SPY, IWM, QQQ, DIA), during the month of July 2021:\n\npd.options.display.max_rows = 25\ndf_etf = pdr.get_data_yahoo(['SPY', 'QQQ', 'IWM', 'DIA'], start='2021-06-30', end='2021-07-31')\ndf_etf = df_etf.round(2)\ndf_etf.head()\n\n\n\n\n\n  \n    \n      Attributes\n      Adj Close\n      Close\n      High\n      ...\n      Low\n      Open\n      Volume\n    \n    \n      Symbols\n      SPY\n      QQQ\n      IWM\n      DIA\n      SPY\n      QQQ\n      IWM\n      DIA\n      SPY\n      QQQ\n      ...\n      IWM\n      DIA\n      SPY\n      QQQ\n      IWM\n      DIA\n      SPY\n      QQQ\n      IWM\n      DIA\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-06-30\n      428.06\n      354.43\n      229.37\n      344.75\n      428.06\n      354.43\n      229.37\n      344.95\n      428.78\n      355.23\n      ...\n      227.76\n      342.35\n      427.21\n      354.83\n      228.65\n      342.38\n      64827900.0\n      32724000.0\n      26039000.0\n      3778900.0\n    \n    \n      2021-07-01\n      430.43\n      354.57\n      231.39\n      346.16\n      430.43\n      354.57\n      231.39\n      346.36\n      430.60\n      355.09\n      ...\n      229.71\n      344.92\n      428.87\n      354.07\n      230.81\n      345.78\n      53441000.0\n      29290000.0\n      18089100.0\n      3606900.0\n    \n    \n      2021-07-02\n      433.72\n      358.64\n      229.19\n      347.73\n      433.72\n      358.64\n      229.19\n      347.94\n      434.10\n      358.97\n      ...\n      228.56\n      346.18\n      431.67\n      356.52\n      232.00\n      347.04\n      57697700.0\n      32727200.0\n      21029700.0\n      3013500.0\n    \n    \n      2021-07-06\n      432.93\n      360.19\n      225.86\n      345.62\n      432.93\n      360.19\n      225.86\n      345.82\n      434.01\n      360.48\n      ...\n      223.87\n      343.60\n      433.78\n      359.26\n      229.36\n      347.75\n      68710400.0\n      38842400.0\n      27771300.0\n      3910600.0\n    \n    \n      2021-07-07\n      434.46\n      360.95\n      223.76\n      346.71\n      434.46\n      360.95\n      223.76\n      346.92\n      434.76\n      362.76\n      ...\n      221.80\n      344.43\n      433.66\n      362.45\n      225.54\n      345.65\n      63549500.0\n      35265200.0\n      28521500.0\n      3347000.0\n    \n  \n\n5 rows × 24 columns\n\n\n\nThis data is not as tidy as we would like. Let’s use method chaining to perform a series of data munging operations.\n\ndf_etf = \\\n    (\n    df_etf\n        .stack(level='Symbols') #pivot the table\n        .reset_index() #turn date into a column \n        .sort_values(by=['Symbols', 'Date']) #sort\n        .rename(columns={'Date':'date', 'Symbols':'symbol', 'Adj Close':'adj_close','Close':'close', \n                         'High':'high', 'Low':'low', 'Open':'open', 'Volume':'volume'}) #renaming columns\n        [['date', 'symbol','open', 'high', 'low', 'close', 'volume', 'adj_close']] #reordering columns\n    )\ndf_etf\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      3\n      2021-06-30\n      DIA\n      342.38\n      345.51\n      342.35\n      344.95\n      3778900.0\n      344.75\n    \n    \n      7\n      2021-07-01\n      DIA\n      345.78\n      346.40\n      344.92\n      346.36\n      3606900.0\n      346.16\n    \n    \n      11\n      2021-07-02\n      DIA\n      347.04\n      348.29\n      346.18\n      347.94\n      3013500.0\n      347.73\n    \n    \n      15\n      2021-07-06\n      DIA\n      347.75\n      348.11\n      343.60\n      345.82\n      3910600.0\n      345.62\n    \n    \n      19\n      2021-07-07\n      DIA\n      345.65\n      347.14\n      344.43\n      346.92\n      3347000.0\n      346.71\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      68\n      2021-07-26\n      SPY\n      439.31\n      441.03\n      439.26\n      441.02\n      43719200.0\n      441.02\n    \n    \n      72\n      2021-07-27\n      SPY\n      439.91\n      439.94\n      435.99\n      439.01\n      67397100.0\n      439.01\n    \n    \n      76\n      2021-07-28\n      SPY\n      439.68\n      440.30\n      437.31\n      438.83\n      52472400.0\n      438.83\n    \n    \n      80\n      2021-07-29\n      SPY\n      439.82\n      441.80\n      439.81\n      440.65\n      47435300.0\n      440.65\n    \n    \n      84\n      2021-07-30\n      SPY\n      437.91\n      440.06\n      437.77\n      438.51\n      68890600.0\n      438.51\n    \n  \n\n88 rows × 8 columns\n\n\n\n\n4.5.1 Querying for One Symbol\nWe are now ready to apply DataFrame masking to our ETF data set.\nAs a first example, let’s isolate all the rows of df_etf that correspond to IWM:\n\npd.options.display.max_rows = 6\nser_bool = (df_etf['symbol'] == \"IWM\")\ndf_etf[ser_bool]\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      2\n      2021-06-30\n      IWM\n      228.65\n      230.32\n      227.76\n      229.37\n      26039000.0\n      229.37\n    \n    \n      6\n      2021-07-01\n      IWM\n      230.81\n      231.85\n      229.71\n      231.39\n      18089100.0\n      231.39\n    \n    \n      10\n      2021-07-02\n      IWM\n      232.00\n      232.08\n      228.56\n      229.19\n      21029700.0\n      229.19\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      78\n      2021-07-28\n      IWM\n      219.00\n      222.59\n      217.40\n      220.82\n      33043700.0\n      220.82\n    \n    \n      82\n      2021-07-29\n      IWM\n      222.79\n      224.44\n      222.14\n      222.52\n      22634800.0\n      222.52\n    \n    \n      86\n      2021-07-30\n      IWM\n      221.65\n      224.05\n      220.28\n      221.05\n      28465700.0\n      221.05\n    \n  \n\n22 rows × 8 columns\n\n\n\nNotice that we did this in two steps:\n\nCalculate the series of booleans called ser_bool using comparison broadcasting.\nPerform the masking by using square brackets [] and ser_bool.\n\nWe can actually perform this masking in a single line of code (without creating an intermediate variable):\n\ndf_etf[df_etf['symbol'] == \"IWM\"]\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      2\n      2021-06-30\n      IWM\n      228.65\n      230.32\n      227.76\n      229.37\n      26039000.0\n      229.37\n    \n    \n      6\n      2021-07-01\n      IWM\n      230.81\n      231.85\n      229.71\n      231.39\n      18089100.0\n      231.39\n    \n    \n      10\n      2021-07-02\n      IWM\n      232.00\n      232.08\n      228.56\n      229.19\n      21029700.0\n      229.19\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      78\n      2021-07-28\n      IWM\n      219.00\n      222.59\n      217.40\n      220.82\n      33043700.0\n      220.82\n    \n    \n      82\n      2021-07-29\n      IWM\n      222.79\n      224.44\n      222.14\n      222.52\n      22634800.0\n      222.52\n    \n    \n      86\n      2021-07-30\n      IWM\n      221.65\n      224.05\n      220.28\n      221.05\n      28465700.0\n      221.05\n    \n  \n\n22 rows × 8 columns\n\n\n\nCode Challenge: Select all the rows of df_etf for QQQ.\n\ndf_etf[df_etf['symbol'] == 'QQQ']\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      1\n      2021-06-30\n      QQQ\n      354.83\n      355.23\n      353.83\n      354.43\n      32724000.0\n      354.43\n    \n    \n      5\n      2021-07-01\n      QQQ\n      354.07\n      355.09\n      352.68\n      354.57\n      29290000.0\n      354.57\n    \n    \n      9\n      2021-07-02\n      QQQ\n      356.52\n      358.97\n      356.28\n      358.64\n      32727200.0\n      358.64\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      77\n      2021-07-28\n      QQQ\n      365.60\n      367.45\n      363.24\n      365.83\n      42066200.0\n      365.83\n    \n    \n      81\n      2021-07-29\n      QQQ\n      365.25\n      367.68\n      365.25\n      366.48\n      25672500.0\n      366.48\n    \n    \n      85\n      2021-07-30\n      QQQ\n      362.44\n      365.17\n      362.41\n      364.57\n      36463500.0\n      364.57\n    \n  \n\n22 rows × 8 columns\n\n\n\n\n\n4.5.2 Querying for Multiple Symbols\nWe can use the .isin() method to filter a DataFrame for multiple symbols. The technique is to feed .isin() a list of symbols you want to filter for.\nThe following code grabs all the rows of df_etf for both QQQ and DIA:\n\ndf_etf[df_etf['symbol'].isin(['QQQ', 'DIA'])]\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      3\n      2021-06-30\n      DIA\n      342.38\n      345.51\n      342.35\n      344.95\n      3778900.0\n      344.75\n    \n    \n      7\n      2021-07-01\n      DIA\n      345.78\n      346.40\n      344.92\n      346.36\n      3606900.0\n      346.16\n    \n    \n      11\n      2021-07-02\n      DIA\n      347.04\n      348.29\n      346.18\n      347.94\n      3013500.0\n      347.73\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      77\n      2021-07-28\n      QQQ\n      365.60\n      367.45\n      363.24\n      365.83\n      42066200.0\n      365.83\n    \n    \n      81\n      2021-07-29\n      QQQ\n      365.25\n      367.68\n      365.25\n      366.48\n      25672500.0\n      366.48\n    \n    \n      85\n      2021-07-30\n      QQQ\n      362.44\n      365.17\n      362.41\n      364.57\n      36463500.0\n      364.57\n    \n  \n\n44 rows × 8 columns\n\n\n\nCode Challenge: Grab all rows of df_etf corresponding to SPY, IWM, and QQQ.\n\ndf_etf[df_etf['symbol'].isin(['SPY', 'IWM', 'QQQ'])]\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      2\n      2021-06-30\n      IWM\n      228.65\n      230.32\n      227.76\n      229.37\n      26039000.0\n      229.37\n    \n    \n      6\n      2021-07-01\n      IWM\n      230.81\n      231.85\n      229.71\n      231.39\n      18089100.0\n      231.39\n    \n    \n      10\n      2021-07-02\n      IWM\n      232.00\n      232.08\n      228.56\n      229.19\n      21029700.0\n      229.19\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      76\n      2021-07-28\n      SPY\n      439.68\n      440.30\n      437.31\n      438.83\n      52472400.0\n      438.83\n    \n    \n      80\n      2021-07-29\n      SPY\n      439.82\n      441.80\n      439.81\n      440.65\n      47435300.0\n      440.65\n    \n    \n      84\n      2021-07-30\n      SPY\n      437.91\n      440.06\n      437.77\n      438.51\n      68890600.0\n      438.51\n    \n  \n\n66 rows × 8 columns\n\n\n\n\n\n4.5.3 Querying for Dates\nThe following code grabs all the rows of df_etf that come after the middle of the month:\n\ndf_etf[df_etf['date'] > '2021-07-15']\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      47\n      2021-07-16\n      DIA\n      350.72\n      350.74\n      346.34\n      346.74\n      5710400.0\n      346.74\n    \n    \n      51\n      2021-07-19\n      DIA\n      341.79\n      350.03\n      337.38\n      339.88\n      9715300.0\n      339.88\n    \n    \n      55\n      2021-07-20\n      DIA\n      340.29\n      346.12\n      339.75\n      345.08\n      5802200.0\n      345.08\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      76\n      2021-07-28\n      SPY\n      439.68\n      440.30\n      437.31\n      438.83\n      52472400.0\n      438.83\n    \n    \n      80\n      2021-07-29\n      SPY\n      439.82\n      441.80\n      439.81\n      440.65\n      47435300.0\n      440.65\n    \n    \n      84\n      2021-07-30\n      SPY\n      437.91\n      440.06\n      437.77\n      438.51\n      68890600.0\n      438.51\n    \n  \n\n44 rows × 8 columns\n\n\n\nCode Challenge: Grab all the rows of df_etf for the last trade date of the month.\n\ndf_etf[df_etf['date'] == '2021-07-30']\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      87\n      2021-07-30\n      DIA\n      349.88\n      351.01\n      348.67\n      349.48\n      3573000.0\n      349.48\n    \n    \n      86\n      2021-07-30\n      IWM\n      221.65\n      224.05\n      220.28\n      221.05\n      28465700.0\n      221.05\n    \n    \n      85\n      2021-07-30\n      QQQ\n      362.44\n      365.17\n      362.41\n      364.57\n      36463500.0\n      364.57\n    \n    \n      84\n      2021-07-30\n      SPY\n      437.91\n      440.06\n      437.77\n      438.51\n      68890600.0\n      438.51\n    \n  \n\n\n\n\n\n\n4.5.4 Querying on Multiple Criteria\nWe can filter on muliple criteria by using the & operator, which is the vectorized version of and.\nSuppose that we want all rows for SPY that come before July fourth:\n\nbln_ticker = (df_etf['symbol'] == 'SPY')\nbln_date = (df_etf['date'] < '2021-07-04')\nbln_combined = bln_ticker & bln_date\n\ndf_etf[bln_combined]\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      SPY\n      427.21\n      428.78\n      427.18\n      428.06\n      64827900.0\n      428.06\n    \n    \n      4\n      2021-07-01\n      SPY\n      428.87\n      430.60\n      428.80\n      430.43\n      53441000.0\n      430.43\n    \n    \n      8\n      2021-07-02\n      SPY\n      431.67\n      434.10\n      430.52\n      433.72\n      57697700.0\n      433.72\n    \n  \n\n\n\n\nCode Challenge: Isolate the rows for QQQ and IWM on the last trading day before July 4th.\n\ndf_etf[(df_etf['symbol'].isin([\"QQQ\", \"IWM\"])) & (df_etf['date']=='2021-07-02')]\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      10\n      2021-07-02\n      IWM\n      232.00\n      232.08\n      228.56\n      229.19\n      21029700.0\n      229.19\n    \n    \n      9\n      2021-07-02\n      QQQ\n      356.52\n      358.97\n      356.28\n      358.64\n      32727200.0\n      358.64"
  },
  {
    "objectID": "04_query.html#querying-with-.query",
    "href": "04_query.html#querying-with-.query",
    "title": "4  DataFrame Querying",
    "section": "4.6 Querying with .query()",
    "text": "4.6 Querying with .query()\nI find querying a DataFrame via masking to be rather cumbersome.\nI greatly prefer the use of the DataFrame.query() method which uses strings to define queries.\nFor example, the following code grabs all the rows corresponding to IWM.\n\ndf_etf.query('symbol == \"IWM\"')\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      2\n      2021-06-30\n      IWM\n      228.65\n      230.32\n      227.76\n      229.37\n      26039000.0\n      229.37\n    \n    \n      6\n      2021-07-01\n      IWM\n      230.81\n      231.85\n      229.71\n      231.39\n      18089100.0\n      231.39\n    \n    \n      10\n      2021-07-02\n      IWM\n      232.00\n      232.08\n      228.56\n      229.19\n      21029700.0\n      229.19\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      78\n      2021-07-28\n      IWM\n      219.00\n      222.59\n      217.40\n      220.82\n      33043700.0\n      220.82\n    \n    \n      82\n      2021-07-29\n      IWM\n      222.79\n      224.44\n      222.14\n      222.52\n      22634800.0\n      222.52\n    \n    \n      86\n      2021-07-30\n      IWM\n      221.65\n      224.05\n      220.28\n      221.05\n      28465700.0\n      221.05\n    \n  \n\n22 rows × 8 columns\n\n\n\nThis code queries all rows corresponding to QQQ and DIA.\n\ndf_etf.query('symbol in (\"QQQ\", \"DIA\")')\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      3\n      2021-06-30\n      DIA\n      342.38\n      345.51\n      342.35\n      344.95\n      3778900.0\n      344.75\n    \n    \n      7\n      2021-07-01\n      DIA\n      345.78\n      346.40\n      344.92\n      346.36\n      3606900.0\n      346.16\n    \n    \n      11\n      2021-07-02\n      DIA\n      347.04\n      348.29\n      346.18\n      347.94\n      3013500.0\n      347.73\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      77\n      2021-07-28\n      QQQ\n      365.60\n      367.45\n      363.24\n      365.83\n      42066200.0\n      365.83\n    \n    \n      81\n      2021-07-29\n      QQQ\n      365.25\n      367.68\n      365.25\n      366.48\n      25672500.0\n      366.48\n    \n    \n      85\n      2021-07-30\n      QQQ\n      362.44\n      365.17\n      362.41\n      364.57\n      36463500.0\n      364.57\n    \n  \n\n44 rows × 8 columns\n\n\n\nHere we grab the rows corresponding to the first half of July.\n\ndf_etf.query('date < \"2021-07-15\"')\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      3\n      2021-06-30\n      DIA\n      342.38\n      345.51\n      342.35\n      344.95\n      3778900.0\n      344.75\n    \n    \n      7\n      2021-07-01\n      DIA\n      345.78\n      346.40\n      344.92\n      346.36\n      3606900.0\n      346.16\n    \n    \n      11\n      2021-07-02\n      DIA\n      347.04\n      348.29\n      346.18\n      347.94\n      3013500.0\n      347.73\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      28\n      2021-07-12\n      SPY\n      435.43\n      437.35\n      434.97\n      437.08\n      52889600.0\n      437.08\n    \n    \n      32\n      2021-07-13\n      SPY\n      436.24\n      437.84\n      435.31\n      435.59\n      52911300.0\n      435.59\n    \n    \n      36\n      2021-07-14\n      SPY\n      437.40\n      437.92\n      434.91\n      436.24\n      64130400.0\n      436.24\n    \n  \n\n40 rows × 8 columns\n\n\n\nAnd we can filter on multiple criteria via method chaining. Here we grab all the rows fo SPY and IWM from the second half of the month.\n\n(\ndf_etf\n    .query('symbol in (\"SPY\", \"IWM\")')\n    .query('date > \"2021-07-15\"')\n)\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      46\n      2021-07-16\n      IWM\n      219.83\n      219.88\n      214.47\n      214.95\n      36620200.0\n      214.95\n    \n    \n      50\n      2021-07-19\n      IWM\n      210.63\n      214.45\n      209.05\n      211.73\n      58571000.0\n      211.73\n    \n    \n      54\n      2021-07-20\n      IWM\n      212.20\n      219.27\n      211.26\n      218.30\n      40794600.0\n      218.30\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      76\n      2021-07-28\n      SPY\n      439.68\n      440.30\n      437.31\n      438.83\n      52472400.0\n      438.83\n    \n    \n      80\n      2021-07-29\n      SPY\n      439.82\n      441.80\n      439.81\n      440.65\n      47435300.0\n      440.65\n    \n    \n      84\n      2021-07-30\n      SPY\n      437.91\n      440.06\n      437.77\n      438.51\n      68890600.0\n      438.51\n    \n  \n\n22 rows × 8 columns\n\n\n\nCode Challenge: Grab all the rows of df_etf that correspond to the following criteria: 1. SPY 2. first half of month 3. close less than 450\n\n(\ndf_etf\n    .query('symbol == \"SPY\"')\n    .query('date < \"2021-07-15\"')\n    .query('close < 450')\n)\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      0\n      2021-06-30\n      SPY\n      427.21\n      428.78\n      427.18\n      428.06\n      64827900.0\n      428.06\n    \n    \n      4\n      2021-07-01\n      SPY\n      428.87\n      430.60\n      428.80\n      430.43\n      53441000.0\n      430.43\n    \n    \n      8\n      2021-07-02\n      SPY\n      431.67\n      434.10\n      430.52\n      433.72\n      57697700.0\n      433.72\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      28\n      2021-07-12\n      SPY\n      435.43\n      437.35\n      434.97\n      437.08\n      52889600.0\n      437.08\n    \n    \n      32\n      2021-07-13\n      SPY\n      436.24\n      437.84\n      435.31\n      435.59\n      52911300.0\n      435.59\n    \n    \n      36\n      2021-07-14\n      SPY\n      437.40\n      437.92\n      434.91\n      436.24\n      64130400.0\n      436.24\n    \n  \n\n10 rows × 8 columns"
  },
  {
    "objectID": "04_query.html#related-reading",
    "href": "04_query.html#related-reading",
    "title": "4  DataFrame Querying",
    "section": "4.7 Related Reading",
    "text": "4.7 Related Reading\nPDSH - 2.6 - Comparisons, Masks, and Boolean Logic\nPDSH - 2.7 - Fancy Indexing\nPDSH - 3.2 - Data Indexing and Selection\nPDSH - 3.12 - High Performance Pandas"
  },
  {
    "objectID": "05_functions_apply.html#defining-functions",
    "href": "05_functions_apply.html#defining-functions",
    "title": "5  Functions and DataFrame.apply() Method",
    "section": "5.1 Defining Functions",
    "text": "5.1 Defining Functions\nDefining functions in Python is straightforward. They syntax is simply def function_name(arguments):. The following function squares two numbers.\n\ndef square(x):\n    sq = x ** 2\n    return(sq)\n\nLet’s verify that our function works:\n\nprint(square(2))\nprint(square(5))\n\n4\n25\n\n\nCode Challenge: Write a cube() function that cubes a number, and along the way, verify that indentation is required after the def statement.\n\ndef cube(x):\n    cb = x ** 3\n    return(cb)\n\nprint(cube(3))\n\n27"
  },
  {
    "objectID": "05_functions_apply.html#option-payoff-function",
    "href": "05_functions_apply.html#option-payoff-function",
    "title": "5  Functions and DataFrame.apply() Method",
    "section": "5.2 Option Payoff Function",
    "text": "5.2 Option Payoff Function\nLet’s now write a more financially interesting function.\nOptions are insurance contracts that are written on top of an underlying stock, much like car insurance is written on top of your car. There are two types of options: puts and calls. Put options protect you from the stock price going too low, while call options protect you from the stock price going too high. Both types have a feature called a strike price, which acts much like the deductable of your car insurance. Options expire sometime in the future, and the payoff (payout) of the option at the time of the expiration is as follows:\nLet \\(K\\) be the strike price of an option, and let \\(S_{T}\\) price of its underlying at the time of expiration. Then the payoff of each type of option is as follows:\n\ncall: \\(\\max(S_T - K, 0)\\)\nput: \\(\\max(K - S_T, 0)\\)\n\nWe can codify this as follows:\n\ndef option_payoff(cp, strike, upx):\n    if cp == 'call':\n        payoff = max(upx - strike, 0)\n    elif cp == 'put':\n        payoff = max(strike - upx, 0)\n    \n    return payoff\n\nLet’s verify that our function works:\n\nprint(option_payoff(\"call\", 100, 110))\nprint(option_payoff(\"put\", 100, 110))\nprint(option_payoff(\"call\", 100, 90))\nprint(option_payoff(\"put\", 100, 90))\n\n10\n0\n0\n10"
  },
  {
    "objectID": "05_functions_apply.html#loading-packages",
    "href": "05_functions_apply.html#loading-packages",
    "title": "5  Functions and DataFrame.apply() Method",
    "section": "5.3 Loading Packages",
    "text": "5.3 Loading Packages\nLet’s now load the packages that we will need.\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "05_functions_apply.html#reading-in-data",
    "href": "05_functions_apply.html#reading-in-data",
    "title": "5  Functions and DataFrame.apply() Method",
    "section": "5.4 Reading-In Data",
    "text": "5.4 Reading-In Data\nNext, let’s read in a data file called spy_expiring_options.csv.\nThis data set consists of 21 different options on SPY that expired on November 16, 2018.\nThe upx column is the settle price of SPY from that day, and it will be used to calculate the payoff of each of these options.\n\ndf_opt = pd.read_csv(\"spy_expiring_options.csv\")\ndf_opt = df_opt.round(2)\ndf_opt.head()\n\n\n\n\n\n  \n    \n      \n      underlying\n      upx\n      type\n      expiration\n      data_date\n      strike\n    \n  \n  \n    \n      0\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      270.0\n    \n    \n      1\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      270.5\n    \n    \n      2\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      271.0\n    \n    \n      3\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      271.5\n    \n    \n      4\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      272.0"
  },
  {
    "objectID": "05_functions_apply.html#initializing-payoff-columns",
    "href": "05_functions_apply.html#initializing-payoff-columns",
    "title": "5  Functions and DataFrame.apply() Method",
    "section": "5.5 Initializing Payoff Columns",
    "text": "5.5 Initializing Payoff Columns\nOur ultimate objective is to add a column of option payoffs to df_opt. We are going to accomplish this task using two different methods: (1) a for loop; (2) the DataFrame.apply() method.\nAs a first step, let’s add two columns to df_opt, one for each method, and then initialize them both with np.nan, which is a special data type that represents missing numerical data.\n\ndf_opt['payoff_loop'] = np.nan\ndf_opt['payoff_apply'] = np.nan\ndf_opt.head()\n\n\n\n\n\n  \n    \n      \n      underlying\n      upx\n      type\n      expiration\n      data_date\n      strike\n      payoff_loop\n      payoff_apply\n    \n  \n  \n    \n      0\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      270.0\n      NaN\n      NaN\n    \n    \n      1\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      270.5\n      NaN\n      NaN\n    \n    \n      2\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      271.0\n      NaN\n      NaN\n    \n    \n      3\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      271.5\n      NaN\n      NaN\n    \n    \n      4\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      272.0\n      NaN\n      NaN"
  },
  {
    "objectID": "05_functions_apply.html#calculate-option_payoff-via-for-loop",
    "href": "05_functions_apply.html#calculate-option_payoff-via-for-loop",
    "title": "5  Functions and DataFrame.apply() Method",
    "section": "5.6 Calculate option_payoff via for loop",
    "text": "5.6 Calculate option_payoff via for loop\nLet’s iterate through df_opt with a for loop and calculate the payoffs one by one. Notice that we are useing the .at indexer which is specifically designed to grab a single value from a column.\n\nfor ix in df_opt.index:\n    \n    # grabbing data from dataframe\n    opt_type = df_opt.at[ix, 'type']\n    strike = df_opt.at[ix, 'strike']\n    upx = df_opt.at[ix, 'upx']\n    \n    # calculating payoff\n    payoff = option_payoff(opt_type, strike, upx)\n    \n    # writing payoff to dataframe\n    df_opt.at[ix, 'payoff_loop'] = payoff\n      \ndf_opt\n\n\n\n\n\n  \n    \n      \n      underlying\n      upx\n      type\n      expiration\n      data_date\n      strike\n      payoff_loop\n      payoff_apply\n    \n  \n  \n    \n      0\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      270.0\n      0.00\n      NaN\n    \n    \n      1\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      270.5\n      0.00\n      NaN\n    \n    \n      2\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      271.0\n      0.00\n      NaN\n    \n    \n      3\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      271.5\n      0.00\n      NaN\n    \n    \n      4\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      272.0\n      0.00\n      NaN\n    \n    \n      5\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      272.5\n      0.00\n      NaN\n    \n    \n      6\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      273.0\n      0.00\n      NaN\n    \n    \n      7\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      273.5\n      0.00\n      NaN\n    \n    \n      8\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      274.0\n      0.27\n      NaN\n    \n    \n      9\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      274.5\n      0.77\n      NaN\n    \n    \n      10\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      275.0\n      1.27\n      NaN\n    \n    \n      11\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      275.5\n      1.77\n      NaN\n    \n    \n      12\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      276.0\n      2.27\n      NaN\n    \n    \n      13\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      276.5\n      2.77\n      NaN\n    \n    \n      14\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      277.0\n      3.27\n      NaN\n    \n    \n      15\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      277.5\n      3.77\n      NaN\n    \n    \n      16\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      278.0\n      0.00\n      NaN\n    \n    \n      17\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      278.5\n      0.00\n      NaN\n    \n    \n      18\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      279.0\n      0.00\n      NaN\n    \n    \n      19\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      279.5\n      0.00\n      NaN\n    \n    \n      20\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      280.0\n      0.00\n      NaN"
  },
  {
    "objectID": "05_functions_apply.html#calculate-opt_pay-via-.apply",
    "href": "05_functions_apply.html#calculate-opt_pay-via-.apply",
    "title": "5  Functions and DataFrame.apply() Method",
    "section": "5.7 Calculate opt_pay via .apply()",
    "text": "5.7 Calculate opt_pay via .apply()\nThe DataFrame.apply() method allows us to perform these calculations without explicitly iterating through df_opt with a for loop. It is a way to vectorize user defined functions.\nIn order to make use of .apply(), we will have to construct our custom payoff function slightly differently. The following opt_pay() function expects as its argument the row of a DataFrame:\n\ndef opt_pay(row):\n    # reading function inputs from DataFrame row\n    cp = row['type']\n    strike = row['strike']\n    upx = row['upx']\n    \n    # option payoff logic\n    if cp == 'call':\n        payoff = max(upx - strike, 0)\n    elif cp == 'put':\n        payoff = max(strike - upx, 0)\n    \n    return payoff\n\nWe can use .apply() to calculate the payoffs in a single line of code.\n\ndf_opt['payoff_apply'] = df_opt.apply(opt_pay, axis = 1)\ndf_opt\n\n\n\n\n\n  \n    \n      \n      underlying\n      upx\n      type\n      expiration\n      data_date\n      strike\n      payoff_loop\n      payoff_apply\n    \n  \n  \n    \n      0\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      270.0\n      0.00\n      0.00\n    \n    \n      1\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      270.5\n      0.00\n      0.00\n    \n    \n      2\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      271.0\n      0.00\n      0.00\n    \n    \n      3\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      271.5\n      0.00\n      0.00\n    \n    \n      4\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      272.0\n      0.00\n      0.00\n    \n    \n      5\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      272.5\n      0.00\n      0.00\n    \n    \n      6\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      273.0\n      0.00\n      0.00\n    \n    \n      7\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      273.5\n      0.00\n      0.00\n    \n    \n      8\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      274.0\n      0.27\n      0.27\n    \n    \n      9\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      274.5\n      0.77\n      0.77\n    \n    \n      10\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      275.0\n      1.27\n      1.27\n    \n    \n      11\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      275.5\n      1.77\n      1.77\n    \n    \n      12\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      276.0\n      2.27\n      2.27\n    \n    \n      13\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      276.5\n      2.77\n      2.77\n    \n    \n      14\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      277.0\n      3.27\n      3.27\n    \n    \n      15\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      277.5\n      3.77\n      3.77\n    \n    \n      16\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      278.0\n      0.00\n      0.00\n    \n    \n      17\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      278.5\n      0.00\n      0.00\n    \n    \n      18\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      279.0\n      0.00\n      0.00\n    \n    \n      19\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      279.5\n      0.00\n      0.00\n    \n    \n      20\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      280.0\n      0.00\n      0.00\n    \n  \n\n\n\n\nCode Challenge: Add a column to df_opt that identifies if the upx is bigger or smaller than strike. Do this by writing a custom function and then using DataFrame.apply().\n\ndef big_small(row):\n    upx = row['upx']\n    strike = row['strike']\n    \n    if upx >= strike:\n        return('bigger')\n    else:\n        return('smaller')\n\n\ndf_opt['big_small'] = df_opt.apply(big_small, axis = 1)\ndf_opt\n\n\n\n\n\n  \n    \n      \n      underlying\n      upx\n      type\n      expiration\n      data_date\n      strike\n      payoff_loop\n      payoff_apply\n      big_small\n    \n  \n  \n    \n      0\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      270.0\n      0.00\n      0.00\n      bigger\n    \n    \n      1\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      270.5\n      0.00\n      0.00\n      bigger\n    \n    \n      2\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      271.0\n      0.00\n      0.00\n      bigger\n    \n    \n      3\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      271.5\n      0.00\n      0.00\n      bigger\n    \n    \n      4\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      272.0\n      0.00\n      0.00\n      bigger\n    \n    \n      5\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      272.5\n      0.00\n      0.00\n      bigger\n    \n    \n      6\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      273.0\n      0.00\n      0.00\n      bigger\n    \n    \n      7\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      273.5\n      0.00\n      0.00\n      bigger\n    \n    \n      8\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      274.0\n      0.27\n      0.27\n      smaller\n    \n    \n      9\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      274.5\n      0.77\n      0.77\n      smaller\n    \n    \n      10\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      275.0\n      1.27\n      1.27\n      smaller\n    \n    \n      11\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      275.5\n      1.77\n      1.77\n      smaller\n    \n    \n      12\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      276.0\n      2.27\n      2.27\n      smaller\n    \n    \n      13\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      276.5\n      2.77\n      2.77\n      smaller\n    \n    \n      14\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      277.0\n      3.27\n      3.27\n      smaller\n    \n    \n      15\n      SPY\n      273.73\n      put\n      2018-11-16\n      2018-11-16\n      277.5\n      3.77\n      3.77\n      smaller\n    \n    \n      16\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      278.0\n      0.00\n      0.00\n      smaller\n    \n    \n      17\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      278.5\n      0.00\n      0.00\n      smaller\n    \n    \n      18\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      279.0\n      0.00\n      0.00\n      smaller\n    \n    \n      19\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      279.5\n      0.00\n      0.00\n      smaller\n    \n    \n      20\n      SPY\n      273.73\n      call\n      2018-11-16\n      2018-11-16\n      280.0\n      0.00\n      0.00\n      smaller"
  },
  {
    "objectID": "05_functions_apply.html#related-reading",
    "href": "05_functions_apply.html#related-reading",
    "title": "5  Functions and DataFrame.apply() Method",
    "section": "5.8 Related Reading",
    "text": "5.8 Related Reading\nWTP - 8 - Control Flow\nWTP - 9 - Defining Functions"
  },
  {
    "objectID": "06_merge.html#loading-packages",
    "href": "06_merge.html#loading-packages",
    "title": "6  Merging DataFrames",
    "section": "6.1 Loading Packages",
    "text": "6.1 Loading Packages\nLet’s load the packages we will need for this tutorial.\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "06_merge.html#reading-in-data",
    "href": "06_merge.html#reading-in-data",
    "title": "6  Merging DataFrames",
    "section": "6.2 Reading-In Data",
    "text": "6.2 Reading-In Data\nThe data set we are going to use is a list of ETFs that have weekly expiring options. What does that mean? Most stocks or ETFs have exchange traded options that expire every month, and at any given time the monthly expiring options go out about a year. The most liquid underlyings actually have options that expire every week; these weekly expiring options go out about 6-8 weeks.\nThis is a list that is published by the CBOE and it consists of all the ETFs that have weekly options trading.\n\ndf_weekly = pd.read_csv('weekly_etf.csv')\ndf_weekly.head()\n\n\n\n\n\n  \n    \n      \n      ticker\n      name\n    \n  \n  \n    \n      0\n      AMJ\n      JP Morgan Alerian MLP Index ETN\n    \n    \n      1\n      AMLP\n      Alerian MLP ETF\n    \n    \n      2\n      ASHR\n      Xtrackers Harvest CSI 300 China A-Shares ETF\n    \n    \n      3\n      DIA\n      SPDR Dow Jones Ind Av ETF Trust\n    \n    \n      4\n      DUST\n      Direxion Daily Gold Miners Index Bear 3X Shares\n    \n  \n\n\n\n\nThe next data set that we are going to load is a comprehensive list of all ETFs that are trading in the market.\n\ndf_etf = pd.read_csv(\"etf.csv\")\ndf_etf.head()\n\n\n\n\n\n  \n    \n      \n      symbol\n      name\n      issuer\n      expense_ratio\n      aum\n      spread\n      segment\n    \n  \n  \n    \n      0\n      SPY\n      SPDR S&P 500 ETF Trust\n      State Street Global Advisors\n      0.09%\n      $275.42B\n      0.00%\n      Equity: U.S. - Large Cap\n    \n    \n      1\n      IVV\n      iShares Core S&P 500 ETF\n      BlackRock\n      0.04%\n      $155.86B\n      0.01%\n      Equity: U.S. - Large Cap\n    \n    \n      2\n      VTI\n      Vanguard Total Stock Market ETF\n      Vanguard\n      0.04%\n      $103.58B\n      0.01%\n      Equity: U.S. - Total Market\n    \n    \n      3\n      VOO\n      Vanguard S&P 500 ETF\n      Vanguard\n      0.04%\n      $96.91B\n      0.01%\n      Equity: U.S. - Large Cap\n    \n    \n      4\n      EFA\n      iShares MSCI EAFE ETF\n      BlackRock\n      0.32%\n      $72.12B\n      0.01%\n      Equity: Developed Markets Ex-U.S. - Total Market\n    \n  \n\n\n\n\nMotivation: Notice that df_etf has a segment column which df_weekly does not. This segment column contains asset-class information that could be useful for categorizing the weekly ETFs.\nObjective: we want to get the segment column into df_weekly.\nThere are a couple of ways of accomplishing this in pandas and both of them involve the pd.merge() method:\n\ninner-merge\nleft/right-merge (sometimes called outer merge)"
  },
  {
    "objectID": "06_merge.html#inner",
    "href": "06_merge.html#inner",
    "title": "6  Merging DataFrames",
    "section": "6.3 Inner",
    "text": "6.3 Inner\nAs with many of the basic operations in data analysis, it’s easiest to understand inner-merges by digging into an example.\nHere is the line of code that accomplishes most of the work that we want done:\n\npd.merge(df_weekly, df_etf, how='inner', left_on='ticker', right_on='symbol')\n\n\n\n\n\n  \n    \n      \n      ticker\n      name_x\n      symbol\n      name_y\n      issuer\n      expense_ratio\n      aum\n      spread\n      segment\n    \n  \n  \n    \n      0\n      AMJ\n      JP Morgan Alerian MLP Index ETN\n      AMJ\n      J.P. Morgan Alerian MLP Index ETN\n      JPMorgan\n      0.85%\n      $3.45B\n      0.04%\n      Equity: U.S. MLPs\n    \n    \n      1\n      AMLP\n      Alerian MLP ETF\n      AMLP\n      Alerian MLP ETF\n      ALPS\n      0.85%\n      $10.64B\n      0.10%\n      Equity: U.S. MLPs\n    \n    \n      2\n      ASHR\n      Xtrackers Harvest CSI 300 China A-Shares ETF\n      ASHR\n      Xtrackers Harvest CSI 300 China A-Shares ETF\n      Deutsche Bank\n      0.65%\n      $630.14M\n      0.04%\n      Equity: China - Total Market\n    \n    \n      3\n      DIA\n      SPDR Dow Jones Ind Av ETF Trust\n      DIA\n      SPDR Dow Jones Industrial Average ETF Trust\n      State Street Global Advisors\n      0.17%\n      $21.70B\n      0.01%\n      Equity: U.S. - Large Cap\n    \n    \n      4\n      DUST\n      Direxion Daily Gold Miners Index Bear 3X Shares\n      DUST\n      Direxion Daily Gold Miners Index Bear 3x Shares\n      Direxion\n      1.08%\n      $122.21M\n      0.06%\n      Inverse Equity: Global Gold Miners\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      61\n      XLV\n      HEALTH CARE SELECT SECTOR SPDR\n      XLV\n      Health Care Select Sector SPDR Fund\n      State Street Global Advisors\n      0.13%\n      $17.49B\n      0.01%\n      Equity: U.S. Health Care\n    \n    \n      62\n      XLY\n      Consumer Discretionary Select Sector SPDR\n      XLY\n      Consumer Discretionary Select Sector SPDR Fund\n      State Street Global Advisors\n      0.13%\n      $14.35B\n      0.01%\n      Equity: U.S. Consumer Cyclicals\n    \n    \n      63\n      XME\n      SPDR S&P Metals & Mining ETF\n      XME\n      SPDR S&P Metals & Mining ETF\n      State Street Global Advisors\n      0.35%\n      $879.10M\n      0.03%\n      Equity: U.S. Metals & Mining\n    \n    \n      64\n      XOP\n      P Oil & Gas Exploration & Production ETF\n      XOP\n      SPDR S&P Oil & Gas Exploration & Production ETF\n      State Street Global Advisors\n      0.35%\n      $3.06B\n      0.02%\n      Equity: U.S. Oil & Gas Exploration & Production\n    \n    \n      65\n      XRT\n      SPDR S&P Oil & Gas Exploration & Production ETF\n      XRT\n      SPDR S&P Retail ETF\n      State Street Global Advisors\n      0.35%\n      $704.67M\n      0.02%\n      Equity: U.S. Retail\n    \n  \n\n66 rows × 9 columns\n\n\n\nObservations on the syntax:\n\nThe first two arguments of pd.merge() are the two DataFrames we want to merge together. The first DataFrame is the left DataFrame and the second one is the right DataFrame.\nThe how argument defines the type of merge.\nleft_on is the column in the left table that will be used for matching, right_on is the column in the right table that will be used for matching.\n\nObservations on output:\n\nThe output is basically each of the two tables smashed together, however only the rows with matching ticker/symbol are retained in the output. All columns of both tables are included.\ndf_weekly had 67 rows in it, and df_etf had 2,160 row in it. The DataFrame that results from pd.merge() has 66 rows in it.\nNotice that both df_weekly and df_etf have a column called name. In the merged DataFrame, suffixes of _x and _y have been added to the column names to make them unique.\n\nLet’s do a little clean up of our DataFrame so that it’s just the information that we wanted: df_weekly with the segment column added to it. Notice that .merge() is also a DataFrame method, and we use this form to invoke method chaining.\n\ndf_inner = \\\n    (\n    df_weekly\n        .merge(df_etf, how='inner', left_on='ticker', right_on='symbol')\n        [['ticker', 'name_x', 'segment']]\n        .rename(columns={'name_x':'name'})\n    )\ndf_inner\n\n\n\n\n\n  \n    \n      \n      ticker\n      name\n      segment\n    \n  \n  \n    \n      0\n      AMJ\n      JP Morgan Alerian MLP Index ETN\n      Equity: U.S. MLPs\n    \n    \n      1\n      AMLP\n      Alerian MLP ETF\n      Equity: U.S. MLPs\n    \n    \n      2\n      ASHR\n      Xtrackers Harvest CSI 300 China A-Shares ETF\n      Equity: China - Total Market\n    \n    \n      3\n      DIA\n      SPDR Dow Jones Ind Av ETF Trust\n      Equity: U.S. - Large Cap\n    \n    \n      4\n      DUST\n      Direxion Daily Gold Miners Index Bear 3X Shares\n      Inverse Equity: Global Gold Miners\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      61\n      XLV\n      HEALTH CARE SELECT SECTOR SPDR\n      Equity: U.S. Health Care\n    \n    \n      62\n      XLY\n      Consumer Discretionary Select Sector SPDR\n      Equity: U.S. Consumer Cyclicals\n    \n    \n      63\n      XME\n      SPDR S&P Metals & Mining ETF\n      Equity: U.S. Metals & Mining\n    \n    \n      64\n      XOP\n      P Oil & Gas Exploration & Production ETF\n      Equity: U.S. Oil & Gas Exploration & Production\n    \n    \n      65\n      XRT\n      SPDR S&P Oil & Gas Exploration & Production ETF\n      Equity: U.S. Retail\n    \n  \n\n66 rows × 3 columns"
  },
  {
    "objectID": "06_merge.html#left",
    "href": "06_merge.html#left",
    "title": "6  Merging DataFrames",
    "section": "6.4 Left",
    "text": "6.4 Left\nNotice that in the inner-join example from the previous section, the original DataFrame of ETFs with weekly options (df_weekly) had 67 rows, but the merged DataFrame with the segment column added (df_inner) only has 66 rows.\n\nprint(df_weekly.shape)\nprint(df_inner.shape)\n\n(67, 2)\n(66, 3)\n\n\nSo what happened? This means that one of the tickers from df_weekly had no matching symbol in df_eft.\nInner-merges, by design, are only intended to retain rows that have matches in both tables. This may or may not be the desired behavior you are looking for.\nLet’s say that instead we wanted to keep ALL the rows in the left DataFrame, df_weekly, irrespective of whether there is a match in the right DataFrame.\nThis is precisely what a left-merge is. The syntax is the exact same as before except for the how argument is set to 'left'.\n\npd.merge(df_weekly, df_etf, how='left', left_on='ticker', right_on='symbol')\n\n\n\n\n\n  \n    \n      \n      ticker\n      name_x\n      symbol\n      name_y\n      issuer\n      expense_ratio\n      aum\n      spread\n      segment\n    \n  \n  \n    \n      0\n      AMJ\n      JP Morgan Alerian MLP Index ETN\n      AMJ\n      J.P. Morgan Alerian MLP Index ETN\n      JPMorgan\n      0.85%\n      $3.45B\n      0.04%\n      Equity: U.S. MLPs\n    \n    \n      1\n      AMLP\n      Alerian MLP ETF\n      AMLP\n      Alerian MLP ETF\n      ALPS\n      0.85%\n      $10.64B\n      0.10%\n      Equity: U.S. MLPs\n    \n    \n      2\n      ASHR\n      Xtrackers Harvest CSI 300 China A-Shares ETF\n      ASHR\n      Xtrackers Harvest CSI 300 China A-Shares ETF\n      Deutsche Bank\n      0.65%\n      $630.14M\n      0.04%\n      Equity: China - Total Market\n    \n    \n      3\n      DIA\n      SPDR Dow Jones Ind Av ETF Trust\n      DIA\n      SPDR Dow Jones Industrial Average ETF Trust\n      State Street Global Advisors\n      0.17%\n      $21.70B\n      0.01%\n      Equity: U.S. - Large Cap\n    \n    \n      4\n      DUST\n      Direxion Daily Gold Miners Index Bear 3X Shares\n      DUST\n      Direxion Daily Gold Miners Index Bear 3x Shares\n      Direxion\n      1.08%\n      $122.21M\n      0.06%\n      Inverse Equity: Global Gold Miners\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      62\n      XLV\n      HEALTH CARE SELECT SECTOR SPDR\n      XLV\n      Health Care Select Sector SPDR Fund\n      State Street Global Advisors\n      0.13%\n      $17.49B\n      0.01%\n      Equity: U.S. Health Care\n    \n    \n      63\n      XLY\n      Consumer Discretionary Select Sector SPDR\n      XLY\n      Consumer Discretionary Select Sector SPDR Fund\n      State Street Global Advisors\n      0.13%\n      $14.35B\n      0.01%\n      Equity: U.S. Consumer Cyclicals\n    \n    \n      64\n      XME\n      SPDR S&P Metals & Mining ETF\n      XME\n      SPDR S&P Metals & Mining ETF\n      State Street Global Advisors\n      0.35%\n      $879.10M\n      0.03%\n      Equity: U.S. Metals & Mining\n    \n    \n      65\n      XOP\n      P Oil & Gas Exploration & Production ETF\n      XOP\n      SPDR S&P Oil & Gas Exploration & Production ETF\n      State Street Global Advisors\n      0.35%\n      $3.06B\n      0.02%\n      Equity: U.S. Oil & Gas Exploration & Production\n    \n    \n      66\n      XRT\n      SPDR S&P Oil & Gas Exploration & Production ETF\n      XRT\n      SPDR S&P Retail ETF\n      State Street Global Advisors\n      0.35%\n      $704.67M\n      0.02%\n      Equity: U.S. Retail\n    \n  \n\n67 rows × 9 columns\n\n\n\nLet’s put this left-merged table into a DataFrame called df_left, and perform a bit of data munging.\n\ndf_left = \\\n    (\n    df_weekly\n        .merge(df_etf, how='left', left_on='ticker', right_on='symbol')\n        [['ticker', 'name_x', 'segment']]\n        .rename(columns={'name_x':'name'})\n    )\ndf_left\n\n\n\n\n\n  \n    \n      \n      ticker\n      name\n      segment\n    \n  \n  \n    \n      0\n      AMJ\n      JP Morgan Alerian MLP Index ETN\n      Equity: U.S. MLPs\n    \n    \n      1\n      AMLP\n      Alerian MLP ETF\n      Equity: U.S. MLPs\n    \n    \n      2\n      ASHR\n      Xtrackers Harvest CSI 300 China A-Shares ETF\n      Equity: China - Total Market\n    \n    \n      3\n      DIA\n      SPDR Dow Jones Ind Av ETF Trust\n      Equity: U.S. - Large Cap\n    \n    \n      4\n      DUST\n      Direxion Daily Gold Miners Index Bear 3X Shares\n      Inverse Equity: Global Gold Miners\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      62\n      XLV\n      HEALTH CARE SELECT SECTOR SPDR\n      Equity: U.S. Health Care\n    \n    \n      63\n      XLY\n      Consumer Discretionary Select Sector SPDR\n      Equity: U.S. Consumer Cyclicals\n    \n    \n      64\n      XME\n      SPDR S&P Metals & Mining ETF\n      Equity: U.S. Metals & Mining\n    \n    \n      65\n      XOP\n      P Oil & Gas Exploration & Production ETF\n      Equity: U.S. Oil & Gas Exploration & Production\n    \n    \n      66\n      XRT\n      SPDR S&P Oil & Gas Exploration & Production ETF\n      Equity: U.S. Retail\n    \n  \n\n67 rows × 3 columns\n\n\n\nCode Challenge: Use .query() on df_left to verify that ticker FTK has NaNs for all the columns from df_etf. Do this in two separate ways:\n\nquerying on ticker\nquerying on segment\n\n\ndf_left.query('ticker == \"FTK\"')\n\n\n\n\n\n  \n    \n      \n      ticker\n      name\n      segment\n    \n  \n  \n    \n      17\n      FTK\n      FLOTEK INDUSTRIES INC\n      NaN\n    \n  \n\n\n\n\n\ndf_left.query('segment.isnull()')\n\n\n\n\n\n  \n    \n      \n      ticker\n      name\n      segment\n    \n  \n  \n    \n      17\n      FTK\n      FLOTEK INDUSTRIES INC\n      NaN\n    \n  \n\n\n\n\nResearch Challenge: Google FTK and figure out why it’s not in df_etf.\n\n# FTK is a stock not and ETF."
  },
  {
    "objectID": "06_merge.html#related-reading",
    "href": "06_merge.html#related-reading",
    "title": "6  Merging DataFrames",
    "section": "6.5 Related Reading",
    "text": "6.5 Related Reading\nPDSH - 3.7 - Combining Datasets: Merging and Joining"
  },
  {
    "objectID": "07_groupby_aggregation_part_1.html#loading-packages",
    "href": "07_groupby_aggregation_part_1.html#loading-packages",
    "title": "7  .groupby() and .agg() - Part 1",
    "section": "7.1 Loading Packages",
    "text": "7.1 Loading Packages\nLet’s load the packages that we will need for this tutorial.\n\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr"
  },
  {
    "objectID": "07_groupby_aggregation_part_1.html#reading-in-data",
    "href": "07_groupby_aggregation_part_1.html#reading-in-data",
    "title": "7  .groupby() and .agg() - Part 1",
    "section": "7.2 Reading-In Data",
    "text": "7.2 Reading-In Data\nOur analysis will be on the set of of July 2021 prices for SPY, IWM, QQQ, DIA.\nLet’s readin that data with pandas_datareader.\n\npd.options.display.max_rows = 25\ndf_etf = pdr.get_data_yahoo(['SPY', 'QQQ', 'IWM', 'DIA'], start='2021-06-30', end='2021-07-31')\ndf_etf = df_etf.round(2)\ndf_etf.head()\n\n\n\n\n\n  \n    \n      Attributes\n      Adj Close\n      Close\n      High\n      ...\n      Low\n      Open\n      Volume\n    \n    \n      Symbols\n      SPY\n      QQQ\n      IWM\n      DIA\n      SPY\n      QQQ\n      IWM\n      DIA\n      SPY\n      QQQ\n      ...\n      IWM\n      DIA\n      SPY\n      QQQ\n      IWM\n      DIA\n      SPY\n      QQQ\n      IWM\n      DIA\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-06-30\n      428.06\n      354.43\n      229.37\n      344.75\n      428.06\n      354.43\n      229.37\n      344.95\n      428.78\n      355.23\n      ...\n      227.76\n      342.35\n      427.21\n      354.83\n      228.65\n      342.38\n      64827900.0\n      32724000.0\n      26039000.0\n      3778900.0\n    \n    \n      2021-07-01\n      430.43\n      354.57\n      231.39\n      346.16\n      430.43\n      354.57\n      231.39\n      346.36\n      430.60\n      355.09\n      ...\n      229.71\n      344.92\n      428.87\n      354.07\n      230.81\n      345.78\n      53441000.0\n      29290000.0\n      18089100.0\n      3606900.0\n    \n    \n      2021-07-02\n      433.72\n      358.64\n      229.19\n      347.73\n      433.72\n      358.64\n      229.19\n      347.94\n      434.10\n      358.97\n      ...\n      228.56\n      346.18\n      431.67\n      356.52\n      232.00\n      347.04\n      57697700.0\n      32727200.0\n      21029700.0\n      3013500.0\n    \n    \n      2021-07-06\n      432.93\n      360.19\n      225.86\n      345.62\n      432.93\n      360.19\n      225.86\n      345.82\n      434.01\n      360.48\n      ...\n      223.87\n      343.60\n      433.78\n      359.26\n      229.36\n      347.75\n      68710400.0\n      38842400.0\n      27771300.0\n      3910600.0\n    \n    \n      2021-07-07\n      434.46\n      360.95\n      223.76\n      346.71\n      434.46\n      360.95\n      223.76\n      346.92\n      434.76\n      362.76\n      ...\n      221.80\n      344.43\n      433.66\n      362.45\n      225.54\n      345.65\n      63549500.0\n      35265200.0\n      28521500.0\n      3347000.0\n    \n  \n\n5 rows × 24 columns\n\n\n\nThis data is not as tidy as we would like. Let’s use method chaining to perform a series of data munging operations.\n\ndf_etf = \\\n    (\n    df_etf\n        .stack(level='Symbols') #pivot the table\n        .reset_index() #turn date into a column \n        .sort_values(by=['Symbols', 'Date']) #sort\n        .rename(columns={'Date':'date', 'Symbols':'symbol', 'Adj Close':'adj_close','Close':'close', \n                         'High':'high', 'Low':'low', 'Open':'open', 'Volume':'volume'}) #renaming columns\n        [['date', 'symbol','open', 'high', 'low', 'close', 'volume', 'adj_close']] #reordering columns\n    )\ndf_etf\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      3\n      2021-06-30\n      DIA\n      342.38\n      345.51\n      342.35\n      344.95\n      3778900.0\n      344.75\n    \n    \n      7\n      2021-07-01\n      DIA\n      345.78\n      346.40\n      344.92\n      346.36\n      3606900.0\n      346.16\n    \n    \n      11\n      2021-07-02\n      DIA\n      347.04\n      348.29\n      346.18\n      347.94\n      3013500.0\n      347.73\n    \n    \n      15\n      2021-07-06\n      DIA\n      347.75\n      348.11\n      343.60\n      345.82\n      3910600.0\n      345.62\n    \n    \n      19\n      2021-07-07\n      DIA\n      345.65\n      347.14\n      344.43\n      346.92\n      3347000.0\n      346.71\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      68\n      2021-07-26\n      SPY\n      439.31\n      441.03\n      439.26\n      441.02\n      43719200.0\n      441.02\n    \n    \n      72\n      2021-07-27\n      SPY\n      439.91\n      439.94\n      435.99\n      439.01\n      67397100.0\n      439.01\n    \n    \n      76\n      2021-07-28\n      SPY\n      439.68\n      440.30\n      437.31\n      438.83\n      52472400.0\n      438.83\n    \n    \n      80\n      2021-07-29\n      SPY\n      439.82\n      441.80\n      439.81\n      440.65\n      47435300.0\n      440.65\n    \n    \n      84\n      2021-07-30\n      SPY\n      437.91\n      440.06\n      437.77\n      438.51\n      68890600.0\n      438.51\n    \n  \n\n88 rows × 8 columns"
  },
  {
    "objectID": "07_groupby_aggregation_part_1.html#calculating-daily-returns-with-groupby",
    "href": "07_groupby_aggregation_part_1.html#calculating-daily-returns-with-groupby",
    "title": "7  .groupby() and .agg() - Part 1",
    "section": "7.3 Calculating Daily Returns with groupby()",
    "text": "7.3 Calculating Daily Returns with groupby()\nOur ultimate goal is to calculate monthly returns and monthly volatilities for each ETF in df_etf. These quantities are both functions of daily returns. So, our first order of business is to calculate daily returns.\nIn a previous tutorial we calculated daily returns in a simple vectorized fashion. Unfortunately, we can’t use the exact same approach here because there are multiple ETFs in the data set.\nTo overcome this challenge we will use our first application of groupby().\nHere is the .groupby() code that calculates daily returns for each ETF.\n\n# sorting values to get everything in the right order\ndf_etf.sort_values(['symbol', 'date'], inplace=True)\n\n# vectorized return calculation\ndf_etf['ret'] = \\\n    df_etf['close'].groupby(df_etf['symbol']).pct_change()\ndf_etf.head()\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n      ret\n    \n  \n  \n    \n      3\n      2021-06-30\n      DIA\n      342.38\n      345.51\n      342.35\n      344.95\n      3778900.0\n      344.75\n      NaN\n    \n    \n      7\n      2021-07-01\n      DIA\n      345.78\n      346.40\n      344.92\n      346.36\n      3606900.0\n      346.16\n      0.004088\n    \n    \n      11\n      2021-07-02\n      DIA\n      347.04\n      348.29\n      346.18\n      347.94\n      3013500.0\n      347.73\n      0.004562\n    \n    \n      15\n      2021-07-06\n      DIA\n      347.75\n      348.11\n      343.60\n      345.82\n      3910600.0\n      345.62\n      -0.006093\n    \n    \n      19\n      2021-07-07\n      DIA\n      345.65\n      347.14\n      344.43\n      346.92\n      3347000.0\n      346.71\n      0.003181\n    \n  \n\n\n\n\nCode Challenge: If the group_by() worked correctly, we should see a NaN value in the ret column for the first trade-date of each ETF. Use DataFrame.query() to confirm this.\n\ndf_etf.query('ret.isnull()')\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n      ret\n    \n  \n  \n    \n      3\n      2021-06-30\n      DIA\n      342.38\n      345.51\n      342.35\n      344.95\n      3778900.0\n      344.75\n      NaN\n    \n    \n      2\n      2021-06-30\n      IWM\n      228.65\n      230.32\n      227.76\n      229.37\n      26039000.0\n      229.37\n      NaN\n    \n    \n      1\n      2021-06-30\n      QQQ\n      354.83\n      355.23\n      353.83\n      354.43\n      32724000.0\n      354.43\n      NaN\n    \n    \n      0\n      2021-06-30\n      SPY\n      427.21\n      428.78\n      427.18\n      428.06\n      64827900.0\n      428.06\n      NaN"
  },
  {
    "objectID": "07_groupby_aggregation_part_1.html#monthly-return-for-each-symbol",
    "href": "07_groupby_aggregation_part_1.html#monthly-return-for-each-symbol",
    "title": "7  .groupby() and .agg() - Part 1",
    "section": "7.4 Monthly Return for Each symbol",
    "text": "7.4 Monthly Return for Each symbol\nWe’ll now proceed to calculate monthly returns and volatilities for each of the ETFs in our data set. This amounts to first grouping by symbol, and then performing an aggregation calculation on returns.\nLet’s start with monthly returns. As a preliminary step we’ll calculate the daily growth factor in a separate column.\n\ndf_etf['daily_factor'] = 1 + df_etf['ret']\ndf_etf.head()\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n      ret\n      daily_factor\n    \n  \n  \n    \n      3\n      2021-06-30\n      DIA\n      342.38\n      345.51\n      342.35\n      344.95\n      3778900.0\n      344.75\n      NaN\n      NaN\n    \n    \n      7\n      2021-07-01\n      DIA\n      345.78\n      346.40\n      344.92\n      346.36\n      3606900.0\n      346.16\n      0.004088\n      1.004088\n    \n    \n      11\n      2021-07-02\n      DIA\n      347.04\n      348.29\n      346.18\n      347.94\n      3013500.0\n      347.73\n      0.004562\n      1.004562\n    \n    \n      15\n      2021-07-06\n      DIA\n      347.75\n      348.11\n      343.60\n      345.82\n      3910600.0\n      345.62\n      -0.006093\n      0.993907\n    \n    \n      19\n      2021-07-07\n      DIA\n      345.65\n      347.14\n      344.43\n      346.92\n      3347000.0\n      346.71\n      0.003181\n      1.003181\n    \n  \n\n\n\n\nRecall that the monthly growth factor is the product of the daily growth factors. Here is a way to write all that logic in a single line using .groupby() and .agg():\n\ndf_grouped_factor = \\\n    df_etf.groupby(['symbol'])['daily_factor'].agg([np.prod]).reset_index()\ndf_grouped_factor\n\n\n\n\n\n  \n    \n      \n      symbol\n      prod\n    \n  \n  \n    \n      0\n      DIA\n      1.013132\n    \n    \n      1\n      IWM\n      0.963727\n    \n    \n      2\n      QQQ\n      1.028609\n    \n    \n      3\n      SPY\n      1.024412\n    \n  \n\n\n\n\nNotice that pandas isn’t very sophisticated about the name that it gives to the column that stores the aggregation calculation. It just gave it the name prod, which the name of the function that was used in the aggregation calculation. Let’s make df_grouped_factor a bit more readable by renaming that column.\n\ndf_grouped_factor.rename(columns={'prod': 'monthly_factor'}, inplace=True)\ndf_grouped_factor\n\n\n\n\n\n  \n    \n      \n      symbol\n      monthly_factor\n    \n  \n  \n    \n      0\n      DIA\n      1.013132\n    \n    \n      1\n      IWM\n      0.963727\n    \n    \n      2\n      QQQ\n      1.028609\n    \n    \n      3\n      SPY\n      1.024412\n    \n  \n\n\n\n\nAnd finally, recall that the monthly return is calculated by subtracting one from the monthly growth factor.\n\ndf_grouped_factor['monthly_return'] = df_grouped_factor['monthly_factor'] - 1\ndf_grouped_factor[['symbol', 'monthly_return']]\n\n\n\n\n\n  \n    \n      \n      symbol\n      monthly_return\n    \n  \n  \n    \n      0\n      DIA\n      0.013132\n    \n    \n      1\n      IWM\n      -0.036273\n    \n    \n      2\n      QQQ\n      0.028609\n    \n    \n      3\n      SPY\n      0.024412"
  },
  {
    "objectID": "07_groupby_aggregation_part_1.html#monthly-volatility-for-each-symbol",
    "href": "07_groupby_aggregation_part_1.html#monthly-volatility-for-each-symbol",
    "title": "7  .groupby() and .agg() - Part 1",
    "section": "7.5 Monthly Volatility for Each symbol",
    "text": "7.5 Monthly Volatility for Each symbol\nNow let’s calculate the (realized/historical) volatility for each of the ETFs.\nWe once again use .groupby() and .agg() to do this all in a single line of code.\n\ndf_grouped_vol = \\\n    df_etf.groupby(['symbol'])['ret'].agg([np.std]).reset_index()\n\ndf_grouped_vol\n\n\n\n\n\n  \n    \n      \n      symbol\n      std\n    \n  \n  \n    \n      0\n      DIA\n      0.007733\n    \n    \n      1\n      IWM\n      0.014032\n    \n    \n      2\n      QQQ\n      0.006832\n    \n    \n      3\n      SPY\n      0.007152\n    \n  \n\n\n\n\nAgain, let’s rename our aggregation column to something more descriptive.\n\ndf_grouped_vol.rename(columns={'std':'daily_vol'}, inplace=True)\ndf_grouped_vol\n\n\n\n\n\n  \n    \n      \n      symbol\n      daily_vol\n    \n  \n  \n    \n      0\n      DIA\n      0.007733\n    \n    \n      1\n      IWM\n      0.014032\n    \n    \n      2\n      QQQ\n      0.006832\n    \n    \n      3\n      SPY\n      0.007152\n    \n  \n\n\n\n\nWhat we have calculated is a daily volatility, but when practitioners talk about volatility, they typically annualize it. A daily volatility is annualized by multiplying by \\(\\sqrt{252}\\).\n\ndf_grouped_vol['ann_vol'] = df_grouped_vol['daily_vol'] * np.sqrt(252)\ndf_grouped_vol\n\n\n\n\n\n  \n    \n      \n      symbol\n      daily_vol\n      ann_vol\n    \n  \n  \n    \n      0\n      DIA\n      0.007733\n      0.122752\n    \n    \n      1\n      IWM\n      0.014032\n      0.222744\n    \n    \n      2\n      QQQ\n      0.006832\n      0.108455\n    \n    \n      3\n      SPY\n      0.007152\n      0.113542\n    \n  \n\n\n\n\nCode Challenge Use .groupby() and .agg() to calculate the average daily return for each of the ETFs.\n\n(\ndf_etf\n    .groupby(['symbol'])[['ret']].agg(np.mean)\n    .reset_index()\n    .rename(columns={'ret':'daily_avg_ret'})\n)\n\n\n\n\n\n  \n    \n      Attributes\n      symbol\n      daily_avg_ret\n    \n  \n  \n    \n      0\n      DIA\n      0.000650\n    \n    \n      1\n      IWM\n      -0.001665\n    \n    \n      2\n      QQQ\n      0.001366\n    \n    \n      3\n      SPY\n      0.001174"
  },
  {
    "objectID": "07_groupby_aggregation_part_1.html#related-reading",
    "href": "07_groupby_aggregation_part_1.html#related-reading",
    "title": "7  .groupby() and .agg() - Part 1",
    "section": "7.6 Related Reading",
    "text": "7.6 Related Reading\nPDSH - 3.8 - Aggregation and Grouping\nPython for Data Analysis (McKinney) - Chapter 9 (pp 251-274) Data Aggregation and Grouping Operations\nOptions, Futures, and Other Derivatives (Hull) - Chapter 15 (pp 325-329) The Black-Scholes-Merton Model"
  },
  {
    "objectID": "08_groupby_aggregation_part_2.html#loading-packages",
    "href": "08_groupby_aggregation_part_2.html#loading-packages",
    "title": "8  .groupby() and .agg() - Part 2",
    "section": "8.1 Loading Packages",
    "text": "8.1 Loading Packages\nLet’s load the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr"
  },
  {
    "objectID": "08_groupby_aggregation_part_2.html#reading-in-data",
    "href": "08_groupby_aggregation_part_2.html#reading-in-data",
    "title": "8  .groupby() and .agg() - Part 2",
    "section": "8.2 Reading-In Data",
    "text": "8.2 Reading-In Data\nIn this tutorial we will be working with price data for the Select Sector SPDR ETFs. Each of these funds tracks a particular subset (sector) of the SP&500 Index. For example, XLF tracks the financial sector and has major holdings in JP Morgan, Wells Fargo, and Bank of America.\nLet grab the data from Yahoo:\n\npd.options.display.max_rows = 25\nlst_symbols = ['XLY', 'XLP', 'XLE', 'XLF', 'XLV', 'XLI', 'XLB', 'XLRE', 'XLK', 'XLU',]\ndf_etf = pdr.get_data_yahoo(lst_symbols, start='2020-01-01', end='2020-12-31')\ndf_etf = df_etf.round(2)\ndf_etf.head()\n\n\n\n\n\n  \n    \n      Attributes\n      Adj Close\n      ...\n      Volume\n    \n    \n      Symbols\n      XLY\n      XLP\n      XLE\n      XLF\n      XLV\n      XLI\n      XLB\n      XLRE\n      XLK\n      XLU\n      ...\n      XLY\n      XLP\n      XLE\n      XLF\n      XLV\n      XLI\n      XLB\n      XLRE\n      XLK\n      XLU\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      125.12\n      60.05\n      55.63\n      30.04\n      99.66\n      80.85\n      58.88\n      36.34\n      91.81\n      60.67\n      ...\n      6295500.0\n      14460700.0\n      11944700.0\n      28843300.0\n      6277400.0\n      16121300.0\n      7357400.0\n      4380100.0\n      13283500.0\n      19107700.0\n    \n    \n      2020-01-03\n      124.05\n      59.96\n      55.47\n      29.72\n      98.79\n      80.69\n      57.93\n      36.61\n      90.78\n      60.80\n      ...\n      5596400.0\n      26388900.0\n      29502900.0\n      51363600.0\n      8247500.0\n      17571300.0\n      12423200.0\n      3499000.0\n      15011800.0\n      17989300.0\n    \n    \n      2020-01-06\n      124.40\n      60.08\n      55.90\n      29.70\n      99.40\n      80.72\n      57.68\n      36.62\n      90.99\n      60.85\n      ...\n      6411600.0\n      22541700.0\n      22458100.0\n      27956100.0\n      6441800.0\n      16153100.0\n      15764400.0\n      3097200.0\n      7815000.0\n      10444500.0\n    \n    \n      2020-01-07\n      124.20\n      59.62\n      55.75\n      29.51\n      99.21\n      80.55\n      57.61\n      36.22\n      90.95\n      60.77\n      ...\n      9150800.0\n      15607600.0\n      11462500.0\n      39627500.0\n      6335300.0\n      16675400.0\n      20266900.0\n      3550600.0\n      7681800.0\n      13070300.0\n    \n    \n      2020-01-08\n      124.58\n      59.84\n      54.83\n      29.70\n      99.85\n      80.83\n      57.82\n      36.40\n      91.93\n      60.74\n      ...\n      4725900.0\n      11451400.0\n      19021400.0\n      47966600.0\n      7494700.0\n      10677700.0\n      8079600.0\n      5089000.0\n      11627200.0\n      12741400.0\n    \n  \n\n5 rows × 60 columns\n\n\n\nThis data is not as tidy as we would like. Let’s use method chaining to perform a series of data munging operations.\n\ndf_etf = \\\n    (\n    df_etf\n        .stack(level='Symbols') #pivot the table\n        .reset_index() #turn date into a column \n        .sort_values(by=['Symbols', 'Date']) #sort\n        .rename(columns={'Date':'date', 'Symbols':'symbol', 'Adj Close':'adj_close','Close':'close', \n                         'High':'high', 'Low':'low', 'Open':'open', 'Volume':'volume'}) #renaming columns\n        [['date', 'symbol','open', 'high', 'low', 'close', 'volume', 'adj_close']] #reordering columns\n    )\ndf_etf\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n    \n  \n  \n    \n      6\n      2020-01-02\n      XLB\n      61.83\n      61.94\n      60.63\n      60.70\n      7357400.0\n      58.88\n    \n    \n      16\n      2020-01-03\n      XLB\n      60.08\n      60.44\n      59.70\n      59.72\n      12423200.0\n      57.93\n    \n    \n      26\n      2020-01-06\n      XLB\n      59.55\n      59.83\n      59.41\n      59.46\n      15764400.0\n      57.68\n    \n    \n      36\n      2020-01-07\n      XLB\n      59.36\n      59.80\n      59.20\n      59.39\n      20266900.0\n      57.61\n    \n    \n      46\n      2020-01-08\n      XLB\n      59.40\n      59.84\n      59.20\n      59.60\n      8079600.0\n      57.82\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2480\n      2020-12-24\n      XLY\n      157.70\n      158.12\n      157.21\n      157.88\n      1048800.0\n      157.40\n    \n    \n      2490\n      2020-12-28\n      XLY\n      159.42\n      160.32\n      158.60\n      159.68\n      2912400.0\n      159.19\n    \n    \n      2500\n      2020-12-29\n      XLY\n      160.24\n      160.53\n      158.98\n      159.73\n      2431200.0\n      159.24\n    \n    \n      2510\n      2020-12-30\n      XLY\n      160.30\n      160.93\n      160.13\n      160.69\n      2440700.0\n      160.20\n    \n    \n      2520\n      2020-12-31\n      XLY\n      160.71\n      160.98\n      159.93\n      160.78\n      2929600.0\n      160.29\n    \n  \n\n2530 rows × 8 columns\n\n\n\nCoding Challenge: Use a DataFrame attribute to determine the number of rows and columns in df_etf.\n\ndf_etf.shape\n\n(2530, 8)"
  },
  {
    "objectID": "08_groupby_aggregation_part_2.html#exploring-and-cleaning-the-data",
    "href": "08_groupby_aggregation_part_2.html#exploring-and-cleaning-the-data",
    "title": "8  .groupby() and .agg() - Part 2",
    "section": "8.3 Exploring and Cleaning the Data",
    "text": "8.3 Exploring and Cleaning the Data\nAs we can see from the coding challenge, this data set is large (by our standards). Whenever I encounter a new data set that I can’t look at in its entirety, I like to do a bit of exploration via the built-in pandas methods.\nWe know we have a variety of ETFs in our data, but it would be useful to know how many (especially if we were expecting a certain number).\n\nprint(df_etf['symbol'].unique())\nprint(df_etf['symbol'].unique().size)\n\n['XLB' 'XLE' 'XLF' 'XLI' 'XLK' 'XLP' 'XLRE' 'XLU' 'XLV' 'XLY']\n10\n\n\nCoding Challenge: What DataFrame attribute could we use to check the data types of the columns of df_etf?\n\ndf_etf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2530 entries, 6 to 2520\nData columns (total 8 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       2530 non-null   datetime64[ns]\n 1   symbol     2530 non-null   object        \n 2   open       2530 non-null   float64       \n 3   high       2530 non-null   float64       \n 4   low        2530 non-null   float64       \n 5   close      2530 non-null   float64       \n 6   volume     2530 non-null   float64       \n 7   adj_close  2530 non-null   float64       \ndtypes: datetime64[ns](1), float64(6), object(1)\nmemory usage: 177.9+ KB\n\n\nWhen I work with a time series of daily prices, I like to check the first and last trade dates that are represented in the data.\n\nprint(df_etf['date'].min())\nprint(df_etf['date'].max())\n\n2020-01-02 00:00:00\n2020-12-31 00:00:00\n\n\nHere is what we know about our data set thus far:\n\n10 different ETFs are represented.\nPrices are coming from the entirety of 2020.\n\nHere are some things that we aren’t necessarily sure of that would be worth checking in a high-stakes situation:\n\nIs there a row/price for each symbol on each trade date?\nIs there ever more than one row/price for a given symbol on a given trade date?\n\nWe won’t bother answering these questions for the purposes of this tutorial, but these are the types of data-integrity questions I will often try to answer when encountering a new data set."
  },
  {
    "objectID": "08_groupby_aggregation_part_2.html#calculating-daily-returns-with-groupby",
    "href": "08_groupby_aggregation_part_2.html#calculating-daily-returns-with-groupby",
    "title": "8  .groupby() and .agg() - Part 2",
    "section": "8.4 Calculating Daily Returns with groupby()",
    "text": "8.4 Calculating Daily Returns with groupby()\nOur ultimate goal is to calculate monthly returns and monthly volatilities for each ETF in df_etf. These quantities are both functions of daily returns. So, our first order of business is to calculate daily returns.\nIn a previous tutorial we calculated daily returns in a simple vectorized fashion. Unfortunately, we can’t use the exact same approach here because there are multiple ETFs in the data set.\nTo overcome this challenge we will use our first application of .groupby().\nHere is the .groupby() code that calculates daily returns for each ETF.\n\n# sorting values to get everything in the right order\ndf_etf.sort_values(['symbol', 'date'], inplace=True)\n\n# vectorized return calculation\ndf_etf['dly_ret'] = \\\n    df_etf['close'].groupby(df_etf['symbol']).pct_change()\ndf_etf.head()\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      symbol\n      open\n      high\n      low\n      close\n      volume\n      adj_close\n      dly_ret\n    \n  \n  \n    \n      6\n      2020-01-02\n      XLB\n      61.83\n      61.94\n      60.63\n      60.70\n      7357400.0\n      58.88\n      NaN\n    \n    \n      16\n      2020-01-03\n      XLB\n      60.08\n      60.44\n      59.70\n      59.72\n      12423200.0\n      57.93\n      -0.016145\n    \n    \n      26\n      2020-01-06\n      XLB\n      59.55\n      59.83\n      59.41\n      59.46\n      15764400.0\n      57.68\n      -0.004354\n    \n    \n      36\n      2020-01-07\n      XLB\n      59.36\n      59.80\n      59.20\n      59.39\n      20266900.0\n      57.61\n      -0.001177\n    \n    \n      46\n      2020-01-08\n      XLB\n      59.40\n      59.84\n      59.20\n      59.60\n      8079600.0\n      57.82\n      0.003536\n    \n  \n\n\n\n\n\n8.4.1 Adding year and month Columns\nThe ultimate goal is to calculate monthly statistics for each of the ETFs in our data set.\nAs a preliminary step, let’s add a month and year column to the df_etf by utilizing the .dt attribute that pandas provides for date columns.\n\ndf_etf['year'] = df_etf['date'].dt.year\ndf_etf['month'] = df_etf['date'].dt.month\ndf_etf[['date', 'year', 'month']].head()\n\n\n\n\n\n  \n    \n      Attributes\n      date\n      year\n      month\n    \n  \n  \n    \n      6\n      2020-01-02\n      2020\n      1\n    \n    \n      16\n      2020-01-03\n      2020\n      1\n    \n    \n      26\n      2020-01-06\n      2020\n      1\n    \n    \n      36\n      2020-01-07\n      2020\n      1\n    \n    \n      46\n      2020-01-08\n      2020\n      1\n    \n  \n\n\n\n\nLet’s do a quick data-integrity check: There are 10 ETFs in our data set and there are 12 months in a year, so the number of symbol-year-month combinations should be 120.\nThe following code counts the number of rows associated with each symbol-year-month combination and puts that data into a DataFrame.\n\ndf_num_rows = \\\n    df_etf.groupby(['symbol', 'year', 'month']).size().reset_index()\ndf_num_rows.head()\n\n\n\n\n\n  \n    \n      \n      symbol\n      year\n      month\n      0\n    \n  \n  \n    \n      0\n      XLB\n      2020\n      1\n      21\n    \n    \n      1\n      XLB\n      2020\n      2\n      19\n    \n    \n      2\n      XLB\n      2020\n      3\n      22\n    \n    \n      3\n      XLB\n      2020\n      4\n      21\n    \n    \n      4\n      XLB\n      2020\n      5\n      20\n    \n  \n\n\n\n\nCoding Challenge: Confirm that there are the correct number of symbol-year-month combinations in df_num_rows.\n\ndf_num_rows.shape\n\n(120, 4)\n\n\nNow that we’ve added the year and month columns we can proceed to calculating our monthly statistics."
  },
  {
    "objectID": "08_groupby_aggregation_part_2.html#average-daily-volume",
    "href": "08_groupby_aggregation_part_2.html#average-daily-volume",
    "title": "8  .groupby() and .agg() - Part 2",
    "section": "8.5 Average Daily Volume",
    "text": "8.5 Average Daily Volume\nLet’s start with the most straight-forward calculation: average daily volume, over each month, for all 10 of the ETFs in our data set.\nThis amounts to:\n\ngrouping by symbol, month, and year\napplying the built-in np.mean() function to the volume column\n\n\ndf_volume = \\\n    df_etf.groupby(['symbol', 'year', 'month'])['volume'].agg([np.mean]).reset_index()\ndf_volume.rename(columns={'mean':'avg_volume'}, inplace=True)\ndf_volume.head()\n\n\n\n\n\n  \n    \n      \n      symbol\n      year\n      month\n      avg_volume\n    \n  \n  \n    \n      0\n      XLB\n      2020\n      1\n      7.235429e+06\n    \n    \n      1\n      XLB\n      2020\n      2\n      1.058022e+07\n    \n    \n      2\n      XLB\n      2020\n      3\n      1.432920e+07\n    \n    \n      3\n      XLB\n      2020\n      4\n      9.000557e+06\n    \n    \n      4\n      XLB\n      2020\n      5\n      4.829185e+06\n    \n  \n\n\n\n\nCoding Challenge: Calculate the maximum daily volume for each symbol, over the entire year.\n\ndf_etf.groupby(['symbol', 'year'])['volume'].agg([np.max]).reset_index()\n\n\n\n\n\n  \n    \n      \n      symbol\n      year\n      amax\n    \n  \n  \n    \n      0\n      XLB\n      2020\n      30741700.0\n    \n    \n      1\n      XLE\n      2020\n      99356700.0\n    \n    \n      2\n      XLF\n      2020\n      256525000.0\n    \n    \n      3\n      XLI\n      2020\n      79118200.0\n    \n    \n      4\n      XLK\n      2020\n      61727100.0\n    \n    \n      5\n      XLP\n      2020\n      50978800.0\n    \n    \n      6\n      XLRE\n      2020\n      49899800.0\n    \n    \n      7\n      XLU\n      2020\n      90263100.0\n    \n    \n      8\n      XLV\n      2020\n      39561900.0\n    \n    \n      9\n      XLY\n      2020\n      20616100.0"
  },
  {
    "objectID": "08_groupby_aggregation_part_2.html#monthly-returns",
    "href": "08_groupby_aggregation_part_2.html#monthly-returns",
    "title": "8  .groupby() and .agg() - Part 2",
    "section": "8.6 Monthly Returns",
    "text": "8.6 Monthly Returns\nNext, let’s calculate monthly returns for each of the ETFs in our data set. This amounts to:\n\ngrouping by symbol, month, and year\napplying an aggregation function to the daily_returns column\n\nThese are the same two steps that we have done in our previous aggregation examples. However, there is one additional wrinkle that we are going to have to contend with.\nIn the previous section, we used simple built-in aggregation funtions available through numpy, such as np.max and np.mean. Calculating monthly returns from daily returns is a little more complicated.\nThus, we are going to have to first create a custom function for calculating monthly returns from daily returns, and then use this custom function in .agg().\nThe following code defines our monthly returns function in terms of daily returns:\n\ndef monthly_ret(dly_ret):\n    return np.prod(1 + dly_ret) - 1\n\nNow we can apply monthly_ret for all of our ETFs using the following code.\n\ndf_ret = \\\n    df_etf.groupby(['symbol', 'month', 'year'])['dly_ret'].agg([monthly_ret]).reset_index()\ndf_ret.head()\n\n\n\n\n\n  \n    \n      \n      symbol\n      month\n      year\n      monthly_ret\n    \n  \n  \n    \n      0\n      XLB\n      1\n      2020\n      -0.050577\n    \n    \n      1\n      XLB\n      2\n      2020\n      -0.085199\n    \n    \n      2\n      XLB\n      3\n      2020\n      -0.145675\n    \n    \n      3\n      XLB\n      4\n      2020\n      0.151865\n    \n    \n      4\n      XLB\n      5\n      2020\n      0.068813\n    \n  \n\n\n\n\nWe can see from our calculation that in March of 2020 XLB had a monthly return of -14.6%.\nCoding Challenge: Which ETF had the highest single monthly return in all of 2020? What was the month?\n\ndf_ret.sort_values(by=['monthly_ret'], ascending=False)\n\n\n\n\n\n  \n    \n      \n      symbol\n      month\n      year\n      monthly_ret\n    \n  \n  \n    \n      15\n      XLE\n      4\n      2020\n      0.307639\n    \n    \n      22\n      XLE\n      11\n      2020\n      0.279944\n    \n    \n      111\n      XLY\n      4\n      2020\n      0.188825\n    \n    \n      34\n      XLF\n      11\n      2020\n      0.168483\n    \n    \n      46\n      XLI\n      11\n      2020\n      0.160274\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      74\n      XLRE\n      3\n      2020\n      -0.157380\n    \n    \n      20\n      XLE\n      9\n      2020\n      -0.159888\n    \n    \n      38\n      XLI\n      3\n      2020\n      -0.192529\n    \n    \n      26\n      XLF\n      3\n      2020\n      -0.216999\n    \n    \n      14\n      XLE\n      3\n      2020\n      -0.358074\n    \n  \n\n120 rows × 4 columns"
  },
  {
    "objectID": "08_groupby_aggregation_part_2.html#monthly-volatility",
    "href": "08_groupby_aggregation_part_2.html#monthly-volatility",
    "title": "8  .groupby() and .agg() - Part 2",
    "section": "8.7 Monthly Volatility",
    "text": "8.7 Monthly Volatility\nLet’s use a similar process to calculate the monthly volatility for each of the ETFs.\nWe begin by defining a custom function that calculates the monthly volatility from daily returns. Recall that industry convention is to state these volatilities in annualized terms.\n\ndef monthly_vol(dly_ret):\n    return np.std(dly_ret) * np.sqrt(252)\n\nWe can now use our monthly_vol function in to perform an aggregating calculation.\n\ndf_vol = \\\n    df_etf.groupby(['symbol', 'month', 'year'])['dly_ret'].agg([monthly_vol]).reset_index()\ndf_vol.head()\n\n\n\n\n\n  \n    \n      \n      symbol\n      month\n      year\n      monthly_vol\n    \n  \n  \n    \n      0\n      XLB\n      1\n      2020\n      0.150336\n    \n    \n      1\n      XLB\n      2\n      2020\n      0.282201\n    \n    \n      2\n      XLB\n      3\n      2020\n      0.932265\n    \n    \n      3\n      XLB\n      4\n      2020\n      0.503394\n    \n    \n      4\n      XLB\n      5\n      2020\n      0.277311\n    \n  \n\n\n\n\nCoding Challenge: What was the volatility for XLF in December 2018?\n\ndf_vol.query('symbol == \"XLF\" & month == 12')\n\n\n\n\n\n  \n    \n      \n      symbol\n      month\n      year\n      monthly_vol\n    \n  \n  \n    \n      35\n      XLF\n      12\n      2020\n      0.138521"
  },
  {
    "objectID": "08_groupby_aggregation_part_2.html#combining-metrics---inner-join",
    "href": "08_groupby_aggregation_part_2.html#combining-metrics---inner-join",
    "title": "8  .groupby() and .agg() - Part 2",
    "section": "8.8 Combining Metrics - inner join",
    "text": "8.8 Combining Metrics - inner join\nNow, suppose that we want to combine our three metrics into one report - meaning that we want them organized into one DataFrame in an easy to read fashion.\nOne way to do this is to use the pandas.merge() method that we learned in the previous tutorial to join together df_volume (average daily volume), df_ret (monthly returns), and df_vol (monthly volatility).\n\ndf_joined = \\\n    (\n    df_volume\n        .merge(df_ret, on=['symbol', 'year', 'month'])\n        .merge(df_vol, on=['symbol', 'year', 'month'])\n    )\ndf_joined.head()\n\n\n\n\n\n  \n    \n      \n      symbol\n      year\n      month\n      avg_volume\n      monthly_ret\n      monthly_vol\n    \n  \n  \n    \n      0\n      XLB\n      2020\n      1\n      7.235429e+06\n      -0.050577\n      0.150336\n    \n    \n      1\n      XLB\n      2020\n      2\n      1.058022e+07\n      -0.085199\n      0.282201\n    \n    \n      2\n      XLB\n      2020\n      3\n      1.432920e+07\n      -0.145675\n      0.932265\n    \n    \n      3\n      XLB\n      2020\n      4\n      9.000557e+06\n      0.151865\n      0.503394\n    \n    \n      4\n      XLB\n      2020\n      5\n      4.829185e+06\n      0.068813\n      0.277311"
  },
  {
    "objectID": "08_groupby_aggregation_part_2.html#combining-metrics---multiple-aggregation",
    "href": "08_groupby_aggregation_part_2.html#combining-metrics---multiple-aggregation",
    "title": "8  .groupby() and .agg() - Part 2",
    "section": "8.9 Combining Metrics - multiple aggregation",
    "text": "8.9 Combining Metrics - multiple aggregation\nAnother way to combine all our statistics into a single DataFrame is to supply all of our custom aggregation fuctions as arguments to the .agg() function in one shot.\nHere is what that looks like:\n\n# defining aggregations\nagg_funcs = \\\n    {'volume':[np.mean], 'dly_ret':[monthly_ret, monthly_vol]}\n\n# performing all aggregations all three aggregations at once\ndf_joined = \\\n    df_etf.groupby(['symbol', 'month', 'year']).agg(agg_funcs).reset_index()\n\n# looking at the data frame\ndf_joined.head()\n\n\n\n\n\n  \n    \n      \n      symbol\n      month\n      year\n      volume\n      dly_ret\n    \n    \n      \n      \n      \n      \n      mean\n      monthly_ret\n      monthly_vol\n    \n  \n  \n    \n      0\n      XLB\n      1\n      2020\n      7.235429e+06\n      -0.050577\n      0.150336\n    \n    \n      1\n      XLB\n      2\n      2020\n      1.058022e+07\n      -0.085199\n      0.282201\n    \n    \n      2\n      XLB\n      3\n      2020\n      1.432920e+07\n      -0.145675\n      0.932265\n    \n    \n      3\n      XLB\n      4\n      2020\n      9.000557e+06\n      0.151865\n      0.503394\n    \n    \n      4\n      XLB\n      5\n      2020\n      4.829185e+06\n      0.068813\n      0.277311\n    \n  \n\n\n\n\nNotice that the input into the .agg() method is a dict whose elements are pairs that look like:\n'column_name':[list_of_aggregating_functions].\nCode Challenge: Modify the code above to add maximum daily volume to the report.\n\n# defining aggregations\nagg_funcs = \\\n    {'volume':[np.mean, np.max], 'dly_ret':[monthly_ret, monthly_vol]}\n\n# performing all aggregations all three aggregations at once\ndf_joined = \\\n    df_etf.groupby(['symbol', 'month', 'year']).agg(agg_funcs).reset_index()\n\n# looking at the data frame\ndf_joined.head()\n\n\n\n\n\n  \n    \n      \n      symbol\n      month\n      year\n      volume\n      dly_ret\n    \n    \n      \n      \n      \n      \n      mean\n      amax\n      monthly_ret\n      monthly_vol\n    \n  \n  \n    \n      0\n      XLB\n      1\n      2020\n      7.235429e+06\n      20266900.0\n      -0.050577\n      0.150336\n    \n    \n      1\n      XLB\n      2\n      2020\n      1.058022e+07\n      30741700.0\n      -0.085199\n      0.282201\n    \n    \n      2\n      XLB\n      3\n      2020\n      1.432920e+07\n      28390200.0\n      -0.145675\n      0.932265\n    \n    \n      3\n      XLB\n      4\n      2020\n      9.000557e+06\n      30738000.0\n      0.151865\n      0.503394\n    \n    \n      4\n      XLB\n      5\n      2020\n      4.829185e+06\n      7386300.0\n      0.068813\n      0.277311"
  },
  {
    "objectID": "08_groupby_aggregation_part_2.html#related-reading",
    "href": "08_groupby_aggregation_part_2.html#related-reading",
    "title": "8  .groupby() and .agg() - Part 2",
    "section": "8.10 Related Reading",
    "text": "8.10 Related Reading\nPDSH - 3.7 - Combining Datasets: Merging and Joining\nPDSH - 3.8 - Aggregation and Grouping\nPython for Data Analysis (McKinney) - Chapter 9 (pp 251-274) Data Aggregation and Grouping Operations"
  },
  {
    "objectID": "01_jumpstart.html",
    "href": "01_jumpstart.html",
    "title": "1  Python Jumpstart",
    "section": "",
    "text": "The purpose of this tutorial is to introduce Jupyter Notebook files, and to give a glimpse of how to use them to work with financial data.\nIn particular, we will visualize stock index data to observe the leverage effect: when the market suffers losses, prices become more volatile."
  },
  {
    "objectID": "02_dataframe_basics.html",
    "href": "02_dataframe_basics.html",
    "title": "2  DataFrame Basics",
    "section": "",
    "text": "In this tutorial we cover the basics of working with DataFrames in pandas."
  },
  {
    "objectID": "03_index_slice.html",
    "href": "03_index_slice.html",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "",
    "text": "Access specific rows of a DataFrame by their location is referred to as indexing.\nIf you are accessing a sequence of contiguous rows, this action is sometimes called slicing.\nThe purpose of this tutorial is to survey various methods for indexing and slicing in pandas."
  },
  {
    "objectID": "04_query.html",
    "href": "04_query.html",
    "title": "4  DataFrame Querying",
    "section": "",
    "text": "In this tutorial we discuss two ways of querying a DataFrame:"
  },
  {
    "objectID": "05_functions_apply.html",
    "href": "05_functions_apply.html",
    "title": "5  Functions and DataFrame.apply() Method",
    "section": "",
    "text": "We have already discussed how to add a new column to a DataFrame that is a simple function of existing columns.\nSuppose the situation is a little more complicated, and that the column we want to add is some kind of custom (user defined) function of existing columns.\nIn this tutorial we discuss two ways of doing this:\nWe will use a finance task to motivate these two techniques: calculating the payoffs of expiring options."
  }
]